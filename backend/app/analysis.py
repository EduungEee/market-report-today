"""
AI ë¶„ì„ ëª¨ë“ˆ
LangGraph íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ê³  ì‚°ì—…/ì£¼ì‹ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""
import os
import json
import re
from typing import List, Dict, Optional, Tuple
from openai import OpenAI
import google.generativeai as genai
from sqlalchemy.orm import Session
from sqlalchemy import text, and_
from datetime import date, datetime, timedelta
import pytz
import sys
import os as os_module
import httpx

# models ê²½ë¡œ ì¶”ê°€
backend_path = os_module.path.dirname(os_module.path.dirname(os_module.path.abspath(__file__)))
if backend_path not in sys.path:
    sys.path.insert(0, backend_path)

from models.models import NewsArticle, Report, ReportIndustry, ReportStock

# LangGraph íŒŒì´í”„ë¼ì¸ import
try:
    from langgraph.graph import StateGraph, END
    from typing import TypedDict
    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False
    print("âš ï¸  LangGraph íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë¶„ì„ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

def _safe_json_loads(value: object) -> Optional[dict]:
    """
    ì•ˆì „í•˜ê²Œ JSON ë¬¸ìì—´ì„ dictë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        value: dict ë˜ëŠ” JSON ë¬¸ìì—´(ë˜ëŠ” ê·¸ ì™¸ íƒ€ì…)

    Returns:
        dict ë˜ëŠ” None
    """
    if isinstance(value, dict):
        return value
    if isinstance(value, str):
        try:
            parsed = json.loads(value)
            return parsed if isinstance(parsed, dict) else None
        except Exception:
            return None
    return None


def _normalize_analysis_result(analysis_result: Dict) -> Dict:
    """
    LLM ì‘ë‹µ ìŠ¤í‚¤ë§ˆê°€ ì¼ë¶€ ëˆ„ë½/ë³€í˜•ë˜ì–´ë„ í›„ì† ë¡œì§(DB ì €ì¥/ì‘ë‹µ)ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ ì •ê·œí™”í•©ë‹ˆë‹¤.

    - industries / impact_description / buy|hold|sell_candidates ëˆ„ë½ ì‹œ ê¸°ë³¸ê°’ì„ ì±„ì›ë‹ˆë‹¤.
    - buy|hold|sell_candidates ë‚´ë¶€ì˜ stocksë¥¼ industry.stocksë¡œ í¼ì³ ì €ì¥ ë¡œì§ì—ì„œ ëˆ„ë½ë˜ì§€ ì•Šê²Œ í•©ë‹ˆë‹¤.
    """
    if not isinstance(analysis_result, dict):
        return {"summary": "", "industries": []}

    analysis_result.setdefault("summary", "")
    industries = analysis_result.get("industries")
    if not isinstance(industries, list):
        industries = []
        analysis_result["industries"] = industries

    trend_default_by_bucket = {
        "buy_candidates": "up",
        "hold_candidates": "neutral",
        "sell_candidates": "down",
    }

    for industry in industries:
        if not isinstance(industry, dict):
            continue

        impact_desc = _safe_json_loads(industry.get("impact_description")) or {}
        market_summary = impact_desc.get("market_summary")
        if not isinstance(market_summary, dict):
            impact_desc["market_summary"] = {"market_sentiment": "", "key_themes": []}
        else:
            market_summary.setdefault("market_sentiment", "")
            if not isinstance(market_summary.get("key_themes"), list):
                market_summary["key_themes"] = []

        for bucket in ("buy_candidates", "hold_candidates", "sell_candidates"):
            if not isinstance(impact_desc.get(bucket), list):
                impact_desc[bucket] = []

        industry["impact_description"] = impact_desc

        # ê¸°ì¡´ stocksê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆì„ ë•Œ, impact_description í›„ë³´êµ°ì˜ stocksë¥¼ í¼ì³ ì±„ì›€
        existing_stocks = industry.get("stocks")
        if not isinstance(existing_stocks, list):
            existing_stocks = []

        flattened: list[dict] = []
        seen: set[tuple[str, str, str]] = set()

        for bucket in ("buy_candidates", "hold_candidates", "sell_candidates"):
            groups = impact_desc.get(bucket, [])
            if not isinstance(groups, list):
                continue
            for group in groups:
                if not isinstance(group, dict):
                    continue
                group_stocks = group.get("stocks", [])
                if not isinstance(group_stocks, list):
                    continue
                for stock in group_stocks:
                    if not isinstance(stock, dict):
                        continue
                    stock_code = str(stock.get("stock_code", "") or "")
                    stock_name = str(stock.get("stock_name", "") or "")
                    expected_trend = str(
                        stock.get("expected_trend", trend_default_by_bucket[bucket]) or trend_default_by_bucket[bucket]
                    )
                    key = (stock_code, stock_name, expected_trend)
                    if key in seen:
                        continue
                    seen.add(key)
                    reasoning = str(stock.get("reasoning", "") or "")
                    if reasoning and not reasoning.startswith("["):
                        reasoning = f"[{bucket}] {reasoning}"

                    confidence_score = stock.get("confidence_score", 0.5)
                    try:
                        confidence_score = float(confidence_score)
                    except Exception:
                        confidence_score = 0.5

                    flattened.append(
                        {
                            "stock_code": stock_code,
                            "stock_name": stock_name,
                            "expected_trend": expected_trend,
                            "confidence_score": confidence_score,
                            "reasoning": reasoning,
                        }
                    )

        # ê¸°ì¡´ stocksê°€ ì´ë¯¸ ìˆìœ¼ë©´ ìœ ì§€í•˜ë˜, ë¶€ì¡±í•˜ë©´ flatten ê²°ê³¼ë¥¼ ë’¤ì— ë§ë¶™ì„
        if isinstance(existing_stocks, list) and existing_stocks:
            industry["stocks"] = existing_stocks
        else:
            industry["stocks"] = flattened

        # í•„ë“œ ê¸°ë³¸ê°’
        industry.setdefault("industry_name", "")
        industry.setdefault("impact_level", "medium")
        industry.setdefault("trend_direction", "neutral")

    return analysis_result


def get_openai_client():
    """OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì§€ì—° ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    if not OPENAI_API_KEY:
        return None
    return OpenAI(api_key=OPENAI_API_KEY)


def create_query_embedding(query_text: str) -> Optional[List[float]]:
    """
    ë¶„ì„ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì˜ ë²¡í„° ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
    í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
    
    Args:
        query_text: ì„ë² ë”©ì„ ìƒì„±í•  ì¿¼ë¦¬ í…ìŠ¤íŠ¸
    
    Returns:
        ë²¡í„° ì„ë² ë”© ë¦¬ìŠ¤íŠ¸ (1536 ì°¨ì›) ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    if not OPENAI_API_KEY:
        print("âš ï¸  OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¿¼ë¦¬ ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    if not query_text or not query_text.strip():
        print("âš ï¸  ë¹ˆ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¡œëŠ” ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        # ì„ë² ë”©ì€ OpenAIë¥¼ ì‚¬ìš© (GeminiëŠ” ì„ë² ë”© APIë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ)
        client = get_openai_client()
        if not client:
            return None
        
        # text-embedding-3-small ëª¨ë¸ ì‚¬ìš© (1536 ì°¨ì›, news.pyì™€ ë™ì¼)
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=query_text.strip()
        )
        
        embedding = response.data[0].embedding
        print(f"âœ… ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embedding)} ì°¨ì›")
        return embedding
    except Exception as e:
        import traceback
        print(f"âš ï¸  ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        print(f"Traceback: {traceback.format_exc()}")
        return None


def search_similar_news_by_embedding(
    db: Session,
    query_embedding: List[float],
    start_datetime: Optional[datetime] = None,
    end_datetime: Optional[datetime] = None,
    limit: int = 20
) -> List[NewsArticle]:
    """
    ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ ë‚ ì§œ ë²”ìœ„ ë‚´ì—ì„œ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    ë‚ ì§œ í•„í„°ë§ í›„ ë²¡í„° ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ Nê°œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        query_embedding: ê²€ìƒ‰ ì¿¼ë¦¬ ì„ë² ë”© (1536 ì°¨ì›)
        start_datetime: ì‹œì‘ ë‚ ì§œ/ì‹œê°„ (ê¸°ë³¸ê°’: ì „ë‚  06:00:00)
        end_datetime: ì¢…ë£Œ ë‚ ì§œ/ì‹œê°„ (ê¸°ë³¸ê°’: í˜„ì¬ ì‹œê°„)
        limit: ë°˜í™˜í•  ìµœëŒ€ ë‰´ìŠ¤ ê°œìˆ˜ (ê¸°ë³¸ê°’: 20)
    
    Returns:
        ì¡°íšŒëœ NewsArticle ê°ì²´ ë¦¬ìŠ¤íŠ¸ (ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬)
    """
    # í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
    seoul_tz = pytz.timezone('Asia/Seoul')
    now = datetime.now(seoul_tz)
    
    # ê¸°ë³¸ê°’ ì„¤ì •: ì „ë‚  06:00 ~ í˜„ì¬ ì‹œê°„
    if end_datetime is None:
        end_datetime = now
    else:
        if end_datetime.tzinfo is None:
            end_datetime = seoul_tz.localize(end_datetime)
    
    if start_datetime is None:
        yesterday = (now - timedelta(days=1)).replace(hour=6, minute=0, second=0, microsecond=0)
        start_datetime = yesterday
    else:
        if start_datetime.tzinfo is None:
            start_datetime = seoul_tz.localize(start_datetime)
    
    # ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    start_str = start_datetime.isoformat()
    end_str = end_datetime.isoformat()
    
    # ë²¡í„°ë¥¼ PostgreSQL ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    embedding_str = "[" + ",".join(map(str, query_embedding)) + "]"
    
    try:
        sqlalchemy_conn = db.connection()
        raw_conn = None
        if hasattr(sqlalchemy_conn, 'connection'):
            raw_conn = sqlalchemy_conn.connection
            if hasattr(raw_conn, 'driver_connection'):
                raw_conn = raw_conn.driver_connection
        else:
            raw_conn = sqlalchemy_conn
        
        cursor = raw_conn.cursor()
        
        try:
            # ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ (cosine distance ì‚¬ìš©)
            # <=> ì—°ì‚°ìëŠ” cosine distanceë¥¼ ë°˜í™˜ (ì‘ì„ìˆ˜ë¡ ìœ ì‚¬í•¨)
            # published_at í•„ë“œë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ê±°ë‚˜, metadataì˜ published_dateë¥¼ ì‚¬ìš©
            cursor.execute("""
                SELECT id, embedding <=> %s::vector(1536) AS distance
                FROM news_articles
                WHERE embedding IS NOT NULL
                AND (
                    (published_at IS NOT NULL 
                     AND published_at >= %s::timestamp 
                     AND published_at <= %s::timestamp)
                    OR
                    (metadata IS NOT NULL 
                     AND metadata->>'published_date' IS NOT NULL
                     AND (metadata->>'published_date')::timestamp >= %s::timestamp
                     AND (metadata->>'published_date')::timestamp <= %s::timestamp)
                )
                ORDER BY embedding <=> %s::vector(1536)
                LIMIT %s
            """, (embedding_str, start_str, end_str, start_str, end_str, embedding_str, limit))
            
            article_ids = [row[0] for row in cursor.fetchall()]
            articles = db.query(NewsArticle).filter(NewsArticle.id.in_(article_ids)).all() if article_ids else []
            
            print(f"âœ… ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ì™„ë£Œ: {len(articles)}ê°œ (ê¸°ê°„: {start_datetime.strftime('%Y-%m-%d %H:%M')} ~ {end_datetime.strftime('%Y-%m-%d %H:%M')}, ìƒìœ„ {limit}ê°œ)")
            
            # ì—¬ì „íˆ ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ embedding ì¡°ê±´ì„ ì™„í™”í•´ì„œ ì‹œë„
            if not articles:
                print(f"âš ï¸  embedding ì¡°ê±´ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. embedding ì¡°ê±´ì„ ì™„í™”í•˜ì—¬ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                cursor.execute("""
                    SELECT id, 0.0 AS distance
                    FROM news_articles
                    WHERE (
                        (published_at IS NOT NULL 
                         AND published_at >= %s::timestamp 
                         AND published_at <= %s::timestamp)
                        OR
                        (metadata IS NOT NULL 
                         AND metadata->>'published_date' IS NOT NULL
                         AND (metadata->>'published_date')::timestamp >= %s::timestamp
                         AND (metadata->>'published_date')::timestamp <= %s::timestamp)
                    )
                    ORDER BY COALESCE(published_at, (metadata->>'published_date')::timestamp) DESC
                    LIMIT %s
                """, (start_str, end_str, start_str, end_str, limit))
                
                article_ids = [row[0] for row in cursor.fetchall()]
                articles = db.query(NewsArticle).filter(NewsArticle.id.in_(article_ids)).all() if article_ids else []
                print(f"ğŸ“Š embedding ì¡°ê±´ ì™„í™” í›„ ì¡°íšŒëœ ë‰´ìŠ¤: {len(articles)}ê°œ")
            
            return articles
        finally:
            cursor.close()
        
    except Exception as e:
        import traceback
        print(f"âš ï¸  ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        print(f"Traceback: {traceback.format_exc()}")
        raise ValueError(f"ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


def get_news_by_date_range(
    db: Session,
    start_datetime: Optional[datetime] = None,
    end_datetime: Optional[datetime] = None,
    query_embedding: Optional[List[float]] = None,
    limit: Optional[int] = None
) -> List[NewsArticle]:
    """
    ë²¡í„° DBì—ì„œ ë‚ ì§œ ë²”ìœ„ë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    query_embeddingì´ ì œê³µë˜ë©´ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ì„±ì´ ë†’ì€ ë‰´ìŠ¤ë§Œ ì„ íƒí•©ë‹ˆë‹¤.
    
    Args:
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        start_datetime: ì‹œì‘ ë‚ ì§œ/ì‹œê°„ (ê¸°ë³¸ê°’: ì „ë‚  06:00:00)
        end_datetime: ì¢…ë£Œ ë‚ ì§œ/ì‹œê°„ (ê¸°ë³¸ê°’: í˜„ì¬ ì‹œê°„)
        query_embedding: ê²€ìƒ‰ ì¿¼ë¦¬ ì„ë² ë”© (ì œê³µ ì‹œ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ì‚¬ìš©)
        limit: ë°˜í™˜í•  ìµœëŒ€ ë‰´ìŠ¤ ê°œìˆ˜ (query_embedding ì œê³µ ì‹œ ê¸°ë³¸ê°’: 20, ì—†ìœ¼ë©´ None)
    
    Returns:
        ì¡°íšŒëœ NewsArticle ê°ì²´ ë¦¬ìŠ¤íŠ¸ (embeddingì´ ìˆëŠ” ë‰´ìŠ¤ë§Œ)
    """
    # ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ì‚¬ìš©
    if query_embedding is not None:
        return search_similar_news_by_embedding(
            db=db,
            query_embedding=query_embedding,
            start_datetime=start_datetime,
            end_datetime=end_datetime,
            limit=limit if limit is not None else 20
        )
    
    # ê¸°ì¡´ ë‚ ì§œ ë²”ìœ„ ê²€ìƒ‰ (ë²¡í„° ìœ ì‚¬ë„ ì—†ì´)
    # í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
    seoul_tz = pytz.timezone('Asia/Seoul')
    now = datetime.now(seoul_tz)
    
    # ê¸°ë³¸ê°’ ì„¤ì •: ì „ë‚  06:00 ~ í˜„ì¬ ì‹œê°„
    if end_datetime is None:
        end_datetime = now
    else:
        # timezoneì´ ì—†ìœ¼ë©´ seoul timezone ì¶”ê°€
        if end_datetime.tzinfo is None:
            end_datetime = seoul_tz.localize(end_datetime)
    
    if start_datetime is None:
        # ì „ë‚  06:00:00 ê³„ì‚°
        yesterday = (now - timedelta(days=1)).replace(hour=6, minute=0, second=0, microsecond=0)
        start_datetime = yesterday
    else:
        # timezoneì´ ì—†ìœ¼ë©´ seoul timezone ì¶”ê°€
        if start_datetime.tzinfo is None:
            start_datetime = seoul_tz.localize(start_datetime)
    
    # ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (metadataì˜ published_date í˜•ì‹ê³¼ ì¼ì¹˜)
    start_str = start_datetime.isoformat()
    end_str = end_datetime.isoformat()
    
    # SQL ì¿¼ë¦¬: embeddingì´ NULLì´ ì•„ë‹ˆê³ , metadataì˜ published_dateê°€ ë²”ìœ„ ë‚´ì¸ ë‰´ìŠ¤ ì¡°íšŒ
    # metadata JSONB í•„ë“œì—ì„œ published_date ì¶”ì¶œ ë° í•„í„°ë§
    # published_dateëŠ” ISO í˜•ì‹ ë¬¸ìì—´ì´ë¯€ë¡œ timestampë¡œ ë³€í™˜
    # psycopg2ì˜ raw connectionì„ ì‚¬ìš©í•˜ì—¬ %s ìŠ¤íƒ€ì¼ íŒŒë¼ë¯¸í„° ë°”ì¸ë”© ì‚¬ìš©
    try:
        sqlalchemy_conn = db.connection()
        raw_conn = None
        if hasattr(sqlalchemy_conn, 'connection'):
            raw_conn = sqlalchemy_conn.connection
            if hasattr(raw_conn, 'driver_connection'):
                raw_conn = raw_conn.driver_connection
        else:
            raw_conn = sqlalchemy_conn
        
        cursor = raw_conn.cursor()
        
        try:
            # ë””ë²„ê¹…: ê° ì¡°ê±´ë³„ ë‰´ìŠ¤ ê°œìˆ˜ í™•ì¸
            cursor.execute("SELECT COUNT(*) FROM news_articles")
            total_count = cursor.fetchone()[0]
            print(f"ğŸ“Š ì „ì²´ ë‰´ìŠ¤ ê°œìˆ˜: {total_count}ê°œ")
            
            cursor.execute("SELECT COUNT(*) FROM news_articles WHERE embedding IS NOT NULL")
            with_embedding = cursor.fetchone()[0]
            print(f"ğŸ“Š ì„ë² ë”© ìˆëŠ” ë‰´ìŠ¤: {with_embedding}ê°œ")
            
            cursor.execute("SELECT COUNT(*) FROM news_articles WHERE metadata IS NOT NULL")
            with_metadata = cursor.fetchone()[0]
            print(f"ğŸ“Š ë©”íƒ€ë°ì´í„° ìˆëŠ” ë‰´ìŠ¤: {with_metadata}ê°œ")
            
            cursor.execute("SELECT COUNT(*) FROM news_articles WHERE metadata->>'published_date' IS NOT NULL")
            with_published_date = cursor.fetchone()[0]
            print(f"ğŸ“Š published_date ìˆëŠ” ë‰´ìŠ¤: {with_published_date}ê°œ")
            
            # ë‚ ì§œ ë²”ìœ„ í™•ì¸
            cursor.execute("""
                SELECT COUNT(*) FROM news_articles
                WHERE embedding IS NOT NULL
                AND metadata IS NOT NULL
                AND metadata->>'published_date' IS NOT NULL
            """)
            with_all_conditions = cursor.fetchone()[0]
            print(f"ğŸ“Š ëª¨ë“  ì¡°ê±´ ë§Œì¡± ë‰´ìŠ¤: {with_all_conditions}ê°œ")
            
            # LIMIT ì ˆ ì¶”ê°€ (ì œê³µëœ ê²½ìš°)
            limit_clause = f"LIMIT {limit}" if limit is not None else ""
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ë‰´ìŠ¤ë¥¼ ë°˜ë“œì‹œ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì¡°ê±´ì„ ë‹¨ìˆœí™”
            # published_at í•„ë“œë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ metadataì˜ published_date ì‚¬ìš©
            # ë‚ ì§œ ë²”ìœ„ ì¡°ê±´ì´ ìˆìœ¼ë©´ ì ìš©, ì—†ìœ¼ë©´ ëª¨ë“  ë‰´ìŠ¤ ì¡°íšŒ
            cursor.execute(f"""
                SELECT id FROM news_articles
                WHERE (
                    (published_at IS NOT NULL 
                     AND published_at >= %s::timestamp 
                     AND published_at <= %s::timestamp)
                    OR
                    (metadata IS NOT NULL 
                     AND metadata->>'published_date' IS NOT NULL
                     AND (metadata->>'published_date')::timestamp >= %s::timestamp
                     AND (metadata->>'published_date')::timestamp <= %s::timestamp)
                    OR
                    (published_at IS NULL 
                     AND (metadata IS NULL OR metadata->>'published_date' IS NULL)
                     AND collected_at >= %s::timestamp 
                     AND collected_at <= %s::timestamp)
                )
                ORDER BY COALESCE(published_at, (metadata->>'published_date')::timestamp, collected_at) DESC
                {limit_clause}
            """, (start_str, end_str, start_str, end_str, start_str, end_str))
            
            article_ids = [row[0] for row in cursor.fetchall()]
            articles = db.query(NewsArticle).filter(NewsArticle.id.in_(article_ids)).all() if article_ids else []
            
            # ì—¬ì „íˆ ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ë‚ ì§œ ì¡°ê±´ì„ ì™„í™”í•´ì„œ ìµœê·¼ ë‰´ìŠ¤ ì¡°íšŒ
            if not articles:
                print(f"âš ï¸  ë‚ ì§œ ë²”ìœ„ ë‚´ ë‰´ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìµœê·¼ ë‰´ìŠ¤ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.")
                cursor.execute(f"""
                    SELECT id FROM news_articles
                    ORDER BY COALESCE(published_at, (metadata->>'published_date')::timestamp, collected_at) DESC
                    {limit_clause}
                """)
                
                article_ids = [row[0] for row in cursor.fetchall()]
                articles = db.query(NewsArticle).filter(NewsArticle.id.in_(article_ids)).all() if article_ids else []
                print(f"ğŸ“Š ë‚ ì§œ ì¡°ê±´ ì™„í™” í›„ ì¡°íšŒëœ ë‰´ìŠ¤: {len(articles)}ê°œ")
            
            print(f"âœ… ë²¡í„° DBì—ì„œ ë‰´ìŠ¤ ì¡°íšŒ ì™„ë£Œ: {len(articles)}ê°œ (ê¸°ê°„: {start_datetime.strftime('%Y-%m-%d %H:%M')} ~ {end_datetime.strftime('%Y-%m-%d %H:%M')})")
            
            if not articles:
                # ë‚ ì§œ ë²”ìœ„ ë°–ì˜ ë‰´ìŠ¤ë„ í™•ì¸
                cursor.execute("""
                    SELECT MIN((metadata->>'published_date')::timestamp) as min_date,
                           MAX((metadata->>'published_date')::timestamp) as max_date
                    FROM news_articles
                    WHERE embedding IS NOT NULL
                    AND metadata IS NOT NULL
                    AND metadata->>'published_date' IS NOT NULL
                """)
                date_range = cursor.fetchone()
                if date_range and date_range[0] and date_range[1]:
                    print(f"ğŸ“… DBì— ìˆëŠ” ë‰´ìŠ¤ ë‚ ì§œ ë²”ìœ„: {date_range[0]} ~ {date_range[1]}")
                    print(f"ğŸ“… ì¡°íšŒ ì‹œë„í•œ ë‚ ì§œ ë²”ìœ„: {start_datetime} ~ {end_datetime}")
            
            return articles
        finally:
            cursor.close()
        
    except Exception as e:
        import traceback
        print(f"âš ï¸  ë²¡í„° DB ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        print(f"Traceback: {traceback.format_exc()}")
        raise ValueError(f"ë²¡í„° DBì—ì„œ ë‰´ìŠ¤ë¥¼ ì¡°íšŒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")


def analyze_news_with_ai(news_articles: List[NewsArticle]) -> Dict:
    """
    ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ AIë¡œ ë¶„ì„í•˜ì—¬ íŒŒê¸‰íš¨ê³¼, ì‚°ì—…, ì£¼ì‹ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    LLMì´ ì°¸ì¡°í•œ ê¸°ì‚¬ë¥¼ ì¶”ì í•  ìˆ˜ ìˆë„ë¡ ì œëª©, URL, ë°œí–‰ì¼ì„ í¬í•¨í•©ë‹ˆë‹¤.
    
    Args:
        news_articles: ë¶„ì„í•  ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 20ê°œê¹Œì§€ ë¶„ì„)
    
    Returns:
        ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬. ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¦…ë‹ˆë‹¤:
        
        {
            "summary": str,  # ì „ì²´ ë‰´ìŠ¤ ìš”ì•½ ë° ì£¼ì‹ ë™í–¥ ë¶„ì„ ê·¼ê±° (500~800ì)
            "industries": [
                {
                    "industry_name": str,  # ì‚°ì—…ëª… (ì˜ˆ: "ë°˜ë„ì²´", "ê¸ˆìœµ", "ì—ë„ˆì§€" ë“±)
                    "impact_level": str,  # ì˜í–¥ ìˆ˜ì¤€: "high" | "medium" | "low"
                    "impact_description": str,  # í•´ë‹¹ ì‚°ì—…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…
                    "trend_direction": str,  # íŠ¸ë Œë“œ ë°©í–¥: "positive" | "negative" | "neutral"
                    "stocks": [
                        {
                            "stock_code": str,  # ì¢…ëª©ì½”ë“œ (6ìë¦¬, ì˜ˆ: "005930")
                            "stock_name": str,  # ì¢…ëª©ëª… (ì˜ˆ: "ì‚¼ì„±ì „ì")
                            "expected_trend": str,  # ì˜ˆìƒ íŠ¸ë Œë“œ: "up" | "down" | "neutral"
                            "confidence_score": float,  # ì‹ ë¢°ë„ ì ìˆ˜ (0.0-1.0, ì˜ˆ: 0.85)
                            "reasoning": str  # í•´ë‹¹ ì£¼ì‹ì˜ ì˜ˆì¸¡ ê·¼ê±° ë° ë¶„ì„ ë‚´ìš©
                        }
                    ]
                }
            ]
        }
    
    ë¶„ì„ ê²°ê³¼ ìƒì„¸ ì„¤ëª…:
        - summary: ì „ì²´ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ì¢…í•©í•œ ìš”ì•½ ë° ì£¼ì‹ ë™í–¥ ë¶„ì„ ê·¼ê±° (500~800ì)
          ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ì¢…í•©í•˜ì—¬ ì£¼ì‹ ì‹œì¥ ì „ë°˜ì˜ ë™í–¥ì„ ë¶„ì„í•˜ê³ , ì£¼ìš” ì‚°ì—…ê³¼ ì¢…ëª©ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„
          êµ¬ì²´ì ì¸ ê·¼ê±°ì™€ í•¨ê»˜ ì„¤ëª…í•©ë‹ˆë‹¤. ë‰´ìŠ¤ì—ì„œ ì–¸ê¸‰ëœ êµ¬ì²´ì ì¸ ì‚¬ê±´, ë°ì´í„°, ì •ì±… ë³€í™” ë“±ì„
          ë°”íƒ•ìœ¼ë¡œ ì£¼ì‹ ì‹œì¥ì˜ ì˜ˆìƒ ì›€ì§ì„ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
        - industries: ì‚°ì—…ë³„ ë¶„ì„ ë¦¬ìŠ¤íŠ¸
          - industry_name: ë¶„ì„ëœ ì‚°ì—…ëª… (ì˜ˆ: ë°˜ë„ì²´, ê¸ˆìœµ, ì—ë„ˆì§€, IT, ìë™ì°¨ ë“±)
          - impact_level: ì£¼ì‹ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ìˆ˜ì¤€
            * "high": ë†’ì€ ì˜í–¥ (ì‹œì¥ ë³€ë™ì„± í¬ê²Œ ì¦ê°€)
            * "medium": ì¤‘ê°„ ì˜í–¥ (ì¼ë¶€ ë³€ë™ì„± ì¦ê°€)
            * "low": ë‚®ì€ ì˜í–¥ (ë¯¸ë¯¸í•œ ì˜í–¥)
          - impact_description: í•´ë‹¹ ì‚°ì—…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…
          - trend_direction: íŠ¸ë Œë“œ ë°©í–¥
            * "positive": ê¸ì •ì  íŠ¸ë Œë“œ (ìƒìŠ¹ ì˜ˆìƒ)
            * "negative": ë¶€ì •ì  íŠ¸ë Œë“œ (í•˜ë½ ì˜ˆìƒ)
            * "neutral": ì¤‘ë¦½ì  íŠ¸ë Œë“œ (ë³€ë™ ì—†ìŒ ì˜ˆìƒ)
          - stocks: í•´ë‹¹ ì‚°ì—…ì˜ ì£¼ì‹ ë¦¬ìŠ¤íŠ¸
            * stock_code: 6ìë¦¬ ì¢…ëª©ì½”ë“œ (ì˜ˆ: "005930" = ì‚¼ì„±ì „ì)
            * stock_name: ì¢…ëª©ëª… (ì˜ˆ: "ì‚¼ì„±ì „ì")
            * expected_trend: ì˜ˆìƒ íŠ¸ë Œë“œ
              - "up": ìƒìŠ¹ ì˜ˆìƒ
              - "down": í•˜ë½ ì˜ˆìƒ
              - "neutral": ì¤‘ë¦½ (ë³€ë™ ì—†ìŒ)
            * confidence_score: ì‹ ë¢°ë„ ì ìˆ˜ (0.0-1.0)
              - 0.9 ì´ìƒ: ë§¤ìš° ë†’ì€ ì‹ ë¢°ë„
              - 0.7-0.9: ë†’ì€ ì‹ ë¢°ë„
              - 0.5-0.7: ì¤‘ê°„ ì‹ ë¢°ë„
              - 0.5 ë¯¸ë§Œ: ë‚®ì€ ì‹ ë¢°ë„
            * reasoning: ì˜ˆì¸¡ ê·¼ê±° ë° ë¶„ì„ ë‚´ìš©
    
    Example:
        >>> result = analyze_news_with_ai(news_articles)
        >>> print(result["summary"])
        "ë°˜ë„ì²´ ì‚°ì—… ê¸‰ë“±, ê¸ˆìœµê¶Œ ë¶€ì§„..."
        >>> 
        >>> # ì²« ë²ˆì§¸ ì‚°ì—… ì •ë³´
        >>> industry = result["industries"][0]
        >>> print(industry["industry_name"])  # "ë°˜ë„ì²´"
        >>> print(industry["impact_level"])    # "high"
        >>> print(industry["trend_direction"])  # "positive"
        >>> 
        >>> # ì²« ë²ˆì§¸ ì£¼ì‹ ì •ë³´
        >>> stock = industry["stocks"][0]
        >>> print(stock["stock_code"])  # "005930"
        >>> print(stock["stock_name"])  # "ì‚¼ì„±ì „ì"
        >>> print(stock["expected_trend"])  # "up"
        >>> print(stock["confidence_score"])  # 0.85
        >>> print(stock["reasoning"])  # "ë°˜ë„ì²´ ìˆ˜ìš” ì¦ê°€ë¡œ ì¸í•œ..."
    """
    # Gemini ëª¨ë¸ ì‚¬ìš©
    if not GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # ë‰´ìŠ¤ ìš”ì•½ (ì œëª©, URL, ë°œí–‰ì¼, ë‚´ìš© í¬í•¨)
    news_items = []
    for idx, article in enumerate(news_articles[:20], 1):  # ìµœëŒ€ 20ê°œê¹Œì§€ ë¶„ì„
        # metadataì—ì„œ ì •ë³´ ì¶”ì¶œ
        url = article.url or "URL ì—†ìŒ"
        published_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
        
        if article.article_metadata:
            metadata = article.article_metadata
            if isinstance(metadata, dict):
                url = metadata.get("url", article.url) or "URL ì—†ìŒ"
                published_date = metadata.get("published_date", "ë‚ ì§œ ì •ë³´ ì—†ìŒ")
        
        # published_atì´ ìˆìœ¼ë©´ ì‚¬ìš©
        if article.published_at:
            published_date = article.published_at.strftime("%Y-%m-%d %H:%M:%S")
        
        content_preview = article.content[:500] if article.content else "ë‚´ìš© ì—†ìŒ"
        
        news_items.append(f"""{idx}. ì œëª©: {article.title}
   URL: {url}
   ë°œí–‰ì¼: {published_date}
   ë‚´ìš©: {content_preview}""")
    
    news_summary = "\n\n".join(news_items)

    system_prompt = """
ë‹¹ì‹ ì€ ì¥ê¸°íˆ¬ì ê´€ì ì˜ ì£¼ì‹ ë¦¬ì„œì¹˜ ì• ë„ë¦¬ìŠ¤íŠ¸ì´ì í¬íŠ¸í´ë¦¬ì˜¤ ë§¤ë‹ˆì €ë‹¤.

ì—­í• :
- íˆ¬ì ê¸°ê°„: ìµœì†Œ 3ë…„ ì´ìƒì˜ ì¥ê¸°íˆ¬ì.
- ìŠ¤íƒ€ì¼: ì„±ì¥ì„±ê³¼ ì¬ë¬´ ê±´ì „ì„±ì„ ì¤‘ì‹œí•˜ëŠ” Bottom-up + Top-down í˜¼í•©.
- ëª©í‘œ: ì§€ë‚œ 24ì‹œê°„ ë‰´ìŠ¤ì— ê¸°ë°˜í•´
  1) ìƒˆë¡œ ë§¤ìˆ˜í•  ìœ ë§ ì‚°ì—…êµ°ê³¼ ì¢…ëª©
  2) ê¸°ì¡´ ë³´ìœ  ì‹œ ê³„ì† ë³´ìœ í•  ìœ ë§ ì‚°ì—…êµ°ê³¼ ì¢…ëª©
  3) ë‹¨ê³„ì  ë§¤ë„ë¥¼ ê³ ë ¤í•´ì•¼ í•  ì‚°ì—…êµ°ê³¼ ì¢…ëª©
  ì„ ì‹ë³„í•˜ê³ , ê·¸ ê·¼ê±°ë¥¼ ìš”ì•½í•´ ì œì‹œí•œë‹¤.
- ì¶”ê°€ ëª©í‘œ: ë‰´ìŠ¤ì—ì„œ ë“œëŸ¬ë‚œ 1ì°¨ì (ì§ì ‘) ìˆ˜í˜œ/í”¼í•´ë¿ ì•„ë‹ˆë¼,
  ê³µê¸‰ë§, ê³ ê° ì‚°ì—…, ê²½ìŸ ì‚°ì—…, ëŒ€ì²´ì¬Â·ë³´ì™„ì¬ ê´€ì ì—ì„œ
  2ì°¨Â·3ì°¨ë¡œ íŒŒê¸‰ë˜ëŠ” ì‚°ì—…/ì¢…ëª©ê¹Œì§€ êµ¬ì¡°ì ìœ¼ë¡œ ì˜ˆì¸¡í•œë‹¤.

ì œí•œì‚¬í•­:
- ë‹¨ê¸° ë‰´ìŠ¤ ëª¨ë©˜í…€ë§Œìœ¼ë¡œ ë§¤ìˆ˜/ë§¤ë„ ê²°ì •ì„ ë‚´ë¦¬ì§€ ë§ê³ , ì¥ê¸° êµ¬ì¡°ì  ì„±ì¥ ê°€ëŠ¥ì„±ê³¼ ë¦¬ìŠ¤í¬ë¥¼ í•¨ê»˜ í‰ê°€í•˜ë¼.
- ê³¼ë„í•˜ê²Œ ê³µê²©ì ì´ê±°ë‚˜ íˆ¬ê¸°ì ì¸ í‘œí˜„(â€œë¬´ì¡°ê±´ ì˜¤ë¥¸ë‹¤â€ ë“±)ì€ ê¸ˆì§€í•œë‹¤.
- ë‰´ìŠ¤ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì‚¬ì‹¤ì„ ë‹¨ì •ì ìœ¼ë¡œ ë§Œë“¤ì–´ë‚´ì§€ ë§ê³ ,
  ì—°ì‡„ ì‹œë‚˜ë¦¬ì˜¤ë„ 'í•©ë¦¬ì ì¸ ì¶”ë¡ ' ìˆ˜ì¤€ì—ì„œë§Œ ì œì‹œí•˜ê³ ,
  ë¶ˆí™•ì‹¤ì„±ì´ í¬ë©´ reasoningì— ê·¸ ì‚¬ì‹¤ì„ ëª…ì‹œí•˜ë¼.
- ì‹ ë¢°ë„(confidence_score)ê°€ ë‚®ì€ ê²½ìš°(ì˜ˆ: 0.4 ë¯¸ë§Œ)ì—ëŠ”
  êµ¬ì²´ì ì¸ ë§¤ìˆ˜/ë§¤ë„ë³´ë‹¤ëŠ” ê´€ì°°Â·ëª¨ë‹ˆí„°ë§ ëŒ€ìƒìœ¼ë¡œ ì„œìˆ í•˜ë¼.
- reasoningì˜ ì²« ë¬¸ì¥ì€ ë°˜ë“œì‹œ impact_chain_levelê³¼ propagation_path ë²”ìœ„ë¥¼ ëª…ì‹œí•œë‹¤.
- impact_chain_levelë³´ë‹¤ ë†’ì€ ë‹¨ê³„(ì˜ˆ: 1ì°¨ ì¢…ëª©ì—ì„œ 2ì°¨Â·3ì°¨ ì˜í–¥)ëŠ”
  reasoningê³¼ propagation_path ì–´ë””ì—ë„ ì–¸ê¸‰í•˜ì§€ ì•ŠëŠ”ë‹¤.

ì—°ì‡„ ì˜í–¥ ë¶„ì„ ê¸°ì¤€ (í•„ìˆ˜):
1) 1ì°¨ ì˜í–¥ (confidence_score â‰¥ 0.7 í•„ìˆ˜)
   - ë‰´ìŠ¤ì— ì§ì ‘ ì–¸ê¸‰ëœ ì‚°ì—…/ì¢…ëª©
   - ë˜ëŠ” ë‰´ìŠ¤ì—ì„œ ë“œëŸ¬ë‚œ ì‚¬ê±´ì´ ë§¤ì¶œ/ì´ìµì— ì§ì ‘ ì—°ê²°ë˜ëŠ” ì£¼ì²´
   - ì˜ˆ: "ì‚¼ì„±ì „ì ë°˜ë„ì²´ ë§¤ì¶œ í˜¸ì¡°" â†’ ì‚¼ì„±ì „ì (1ì°¨)

2) 2ì°¨ ì˜í–¥ (confidence_score â‰¥ 0.5 í•„ìˆ˜) 
   - 1ì°¨ ì˜í–¥ì˜ í•µì‹¬ ê³µê¸‰ì—…ì²´/ê³ ê°/íŒŒíŠ¸ë„ˆ
   - ê³µê¸‰ë§ ë¹„ì¤‘ì´ í¬ê±°ë‚˜, ì—­ì‚¬ì  ìƒê´€ê´€ê³„ê°€ ëª…í™•í•œ ê²½ìš°ë§Œ
   - ì˜ˆ: ì‚¼ì„±ì „ì ë°˜ë„ì²´ í˜¸ì¡° â†’ SKí•˜ì´ë‹‰ìŠ¤ HBM (2ì°¨, ì‹¤ì œ ê³ ê°ì‚¬ì„)

3) 3ì°¨ ì˜í–¥ (confidence_score â‰¥ 0.3, ì„ íƒì )
   - 2ì°¨ ì˜í–¥ì˜ ê³µê¸‰ì—…ì²´/ê³ ê° ë˜ëŠ” ì¸í”„ë¼/ìë³¸ì¬
   - ì—­ì‚¬ì  ì‚¬ë¡€ë‚˜ ëª…í™•í•œ ê²½ì œì  ì—°ê²°ê³ ë¦¬ê°€ ìˆì„ ë•Œë§Œ
   - ì˜ˆ: HBM ìˆ˜ìš” ì¦ê°€ â†’ í¬í† ë‹‰ìŠ¤/ì†Œì¬ ì—…ì²´ (3ì°¨)

ì¶œë ¥ í•„ìˆ˜ ê·œì¹™:
- ìµœì†Œ 1ê°œì˜ 1ì°¨ + 1ê°œì˜ 2ì°¨ ì˜í–¥ì€ ë°˜ë“œì‹œ ì œì‹œí•˜ë¼
- 3ì°¨ëŠ” ì‹ ë¢°ë„ â‰¥ 0.3ì´ê³  ë…¼ë¦¬ì  ì—°ê²°ê³ ë¦¬ê°€ ëª…í™•í•  ë•Œë§Œ ì¶”ê°€
- ê° ë‹¨ê³„ë³„ confidence_scoreëŠ” ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë¶€ì—¬í•˜ë¼:
  | ë‹¨ê³„ | ìµœì†Œ ì‹ ë¢°ë„ | ë‰´ìŠ¤ ì§ì ‘ì„± | ì—­ì‚¬ì  ì‚¬ë¡€ |
  |------|-------------|-------------|-------------|
  | 1ì°¨  | â‰¥ 0.7      | ì§ì ‘ ì–¸ê¸‰  | í•„ìš” ì—†ìŒ  |
  | 2ì°¨  | â‰¥ 0.5      | ê°„ì ‘ ì–¸ê¸‰  | ìˆìœ¼ë©´ +0.1|
  | 3ì°¨  | â‰¥ 0.3      | ì¶”ë¡        | ìˆìœ¼ë©´ +0.1|


ì¶œë ¥ í˜•ì‹:
- ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ë¼.
- JSON ì´ì™¸ì˜ ì„¤ëª…, ìì—°ì–´ ë¬¸ì¥, ë§ˆí¬ë‹¤ìš´ì€ ì¶œë ¥í•˜ì§€ ë§ˆë¼.
- ì•„ë˜ ìŠ¤í‚¤ë§ˆë¥¼ ì •í™•íˆ ë”°ë¥´ë¼.

JSON ìŠ¤í‚¤ë§ˆ:
{
  "summary": "ì•„ë˜ ì‚°ì—… ë¶„ì„ì—ëŠ” ì „ì²´ ì‹œì¥ ìš”ì•½, íˆ¬ì ì „ëµ(Buy/Hold/Sell), ì—°ì‡„ ì˜í–¥ ì‹œë‚˜ë¦¬ì˜¤ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",

  "industries": [
    {
      "industry_name": "ì‹œì¥ ì¢…í•© ë° íˆ¬ì ì „ëµ",
      "impact_level": "high" | "medium" | "low",
      "trend_direction": "positive" | "negative" | "neutral",

      "impact_description": {
        "market_summary": {
          "market_sentiment": "positive" | "negative" | "neutral",
          "key_themes": ["string"]
        },

        "buy_candidates": [
          {
            "industry": "string",
            "reason_industry": "string",
            "stocks": [
              {
                "stock_code": "string",
                "stock_name": "string",
                "expected_trend": "up",
                "confidence_score": 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0,
                "impact_chain_level": 1 | 2 | 3,
                "propagation_path": [
                {
                    "level": 1,
                    "details": [
                        "1ì°¨: ë‰´ìŠ¤ ì§ì ‘ ì–¸ê¸‰ â†’ [êµ¬ì²´ì  ì‚¬ê±´]",
                        "íˆ¬ì ë…¼ë¦¬: [ë§¤ì¶œ/ì´ìµ ì˜í–¥ ê²½ë¡œ]",
                        "ì‹ ë¢°ë„ ê·¼ê±°: [ë‰´ìŠ¤ ì§ì ‘ì„±/ìˆ˜ì¹˜ ëª…ì‹œ]",
                    ]
                },
                {
                    "level": 2,
                    "details": [
                        "1ì°¨: [1ì°¨ ì‚°ì—…/ì¢…ëª©] ìˆ˜ìš”/ê³µê¸‰ ë³€í™”",
                        "2ì°¨: [ê³µê¸‰ë§ ì—°ê²°ê³ ë¦¬] â†’ ë³¸ ì¢…ëª© ì˜í–¥", 
                        "ì‹ ë¢°ë„ ê·¼ê±°: [ì—­ì‚¬ì  ì‚¬ë¡€/ë§¤ì¶œ ë¹„ì¤‘/ê³ ê°ì‚¬ ëª…ì‹œ]",
                    ]
                },
                {
                    "level": 3,
                    "details": [
                        "1ì°¨: [1ì°¨ ì‚¬ê±´]",
                        "2ì°¨: [2ì°¨ ì‚°ì—… ì˜í–¥]", 
                        "3ì°¨: [ì¸í”„ë¼/í›„í–‰ ìˆ˜í˜œ] â†’ ë³¸ ì¢…ëª©",
                        "ì‹ ë¢°ë„ ê·¼ê±°: [ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€/íˆ¬ì ì‚¬ì´í´]",
                    ]
                }
                ],
                "reasoning": "string",
                "news_drivers": ["string"],
                "risk_factors": ["string"]
              }
            ]
          }
        ],

        "hold_candidates": [
          {
            "industry": "string",
            "reason_industry": "string",
            "stocks": [
              {
                "stock_code": "string",
                "stock_name": "string",
                "expected_trend": "neutral",
                "confidence_score": 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0,
                "impact_chain_level": 1 | 2 | 3,
                "propagation_path": [
                {
                    "level": 1,
                    "details": [
                        "1ì°¨: ë‰´ìŠ¤ ì§ì ‘ ì–¸ê¸‰ â†’ [êµ¬ì²´ì  ì‚¬ê±´]",
                        "íˆ¬ì ë…¼ë¦¬: [ë§¤ì¶œ/ì´ìµ ì˜í–¥ ê²½ë¡œ]",
                        "ì‹ ë¢°ë„ ê·¼ê±°: [ë‰´ìŠ¤ ì§ì ‘ì„±/ìˆ˜ì¹˜ ëª…ì‹œ]",
                    ]
                },
                {
                    "level": 2,
                    "details": [
                        "1ì°¨: [1ì°¨ ì‚°ì—…/ì¢…ëª©] ìˆ˜ìš”/ê³µê¸‰ ë³€í™”",
                        "2ì°¨: [ê³µê¸‰ë§ ì—°ê²°ê³ ë¦¬] â†’ ë³¸ ì¢…ëª© ì˜í–¥", 
                        "ì‹ ë¢°ë„ ê·¼ê±°: [ì—­ì‚¬ì  ì‚¬ë¡€/ë§¤ì¶œ ë¹„ì¤‘/ê³ ê°ì‚¬ ëª…ì‹œ]",
                    ]
                },
                {
                    "level": 3,
                    "details": [
                        "1ì°¨: [1ì°¨ ì‚¬ê±´]",
                        "2ì°¨: [2ì°¨ ì‚°ì—… ì˜í–¥]", 
                        "3ì°¨: [ì¸í”„ë¼/í›„í–‰ ìˆ˜í˜œ] â†’ ë³¸ ì¢…ëª©",
                        "ì‹ ë¢°ë„ ê·¼ê±°: [ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€/íˆ¬ì ì‚¬ì´í´]",
                    ]
                }
                ],
                "reasoning": "string",
                "news_drivers": ["string"],
                "risk_factors": ["string"]
              }
            ]
          }
        ],

        "sell_candidates": [
          {
            "industry": "string",
            "reason_industry": "string",
            "stocks": [
              {
                "stock_code": "string",
                "stock_name": "string",
                "expected_trend": "down",
                "confidence_score": 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0,
                "impact_chain_level": 1 | 2 | 3,
                "propagation_path": [
                {
                    "level": 1,
                    "details": [
                        "1ì°¨: ë‰´ìŠ¤ ì§ì ‘ ì–¸ê¸‰ â†’ [êµ¬ì²´ì  ì‚¬ê±´]",
                        "íˆ¬ì ë…¼ë¦¬: [ë§¤ì¶œ/ì´ìµ ì˜í–¥ ê²½ë¡œ]",
                        "ì‹ ë¢°ë„ ê·¼ê±°: [ë‰´ìŠ¤ ì§ì ‘ì„±/ìˆ˜ì¹˜ ëª…ì‹œ]",
                    ]
                },
                {
                    "level": 2,
                    "details": [
                        "1ì°¨: [1ì°¨ ì‚°ì—…/ì¢…ëª©] ìˆ˜ìš”/ê³µê¸‰ ë³€í™”",
                        "2ì°¨: [ê³µê¸‰ë§ ì—°ê²°ê³ ë¦¬] â†’ ë³¸ ì¢…ëª© ì˜í–¥", 
                        "ì‹ ë¢°ë„ ê·¼ê±°: [ì—­ì‚¬ì  ì‚¬ë¡€/ë§¤ì¶œ ë¹„ì¤‘/ê³ ê°ì‚¬ ëª…ì‹œ]",
                    ]
                },
                {
                    "level": 3,
                    "details": [
                        "1ì°¨: [1ì°¨ ì‚¬ê±´]",
                        "2ì°¨: [2ì°¨ ì‚°ì—… ì˜í–¥]", 
                        "3ì°¨: [ì¸í”„ë¼/í›„í–‰ ìˆ˜í˜œ] â†’ ë³¸ ì¢…ëª©",
                        "ì‹ ë¢°ë„ ê·¼ê±°: [ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€/íˆ¬ì ì‚¬ì´í´]",
                    ]
                }
                ],
                "reasoning": "string",
                "news_drivers": ["string"],
                "risk_factors": ["string"]
              }
            ]
          }
        ]
      },

      "stocks": []
    }
  ]
}

ì‹œì¥ê³¼ ê°œë³„ ì¢…ëª©ì˜ ê´€ê³„ ê·œì¹™:

1) ì‹œì¥ ë¶„ìœ„ê¸°(market_sentiment)ì˜ ì—­í• 
- "market_sentiment"ëŠ” ì§€ìˆ˜Â·ì‹œì¥ ì „ë°˜ì˜ ìœ„í—˜ ì„ í˜¸/íšŒí”¼ ìƒí™©ì„ ë‚˜íƒ€ë‚¸ë‹¤.
- ì´ëŠ” ê°œë³„ ì¢…ëª©ì˜ ë‹¨ê¸° ìˆ˜ê¸‰ê³¼ ë°¸ë¥˜ì—ì´ì…˜ì— ì˜í–¥ì„ ì£¼ì§€ë§Œ,
  ëª¨ë“  ì¢…ëª©ì— ë™ì¼ ë°©í–¥ì˜ ê²°ë¡ ì„ ê°•ì œë¡œ ì ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.

2) ì—­í–‰ ì‚¬ë¡€ í—ˆìš© (ì¤‘ìš”)
- ì‹œì¥ì´ ë¶€ì •ì ì´ë”ë¼ë„, êµ¬ì¡°ì ìœ¼ë¡œ ì„±ì¥ì„±ì´ í¬ê±°ë‚˜
  í€ë”ë©˜í„¸ì´ ê°œì„ ë˜ëŠ” ì†Œìˆ˜ ì¢…ëª©ì€ "up" ë˜ëŠ” "ë³´ìœ /ì¶”ê°€ ë§¤ìˆ˜" íŒë‹¨ì„ ë‚´ë¦´ ìˆ˜ ìˆë‹¤.
- ë°˜ëŒ€ë¡œ ì‹œì¥ì´ ê¸ì •ì ì´ë”ë¼ë„, ê²½ìŸë ¥ ì•½í™”Â·ê·œì œÂ·ìˆ˜ìš” ê°ì†Œ ë“±ìœ¼ë¡œ
  ì¥ê¸° ì „ë§ì´ ë‚˜ìœ ì¢…ëª©ì€ "down" ë˜ëŠ” "ë§¤ë„/ë¹„ì¤‘ ì¶•ì†Œ" íŒë‹¨ì„ ë‚´ë¦´ ìˆ˜ ìˆë‹¤.
- ì´ëŸ¬í•œ 'ì‹œì¥ê³¼ ë°˜ëŒ€ ë°©í–¥' íŒë‹¨ì„ í•˜ëŠ” ê²½ìš°,
  reasoningì—ì„œ ë°˜ë“œì‹œ ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ëª¨ë‘ ì„¤ëª…í•´ì•¼ í•œë‹¤.
  1) ì‹œì¥ ì „ì²´ì™€ ë‹¤ë¥¸ ê²°ë¡ ì„ ë‚´ë¦° ì´ìœ  (ì¢…ëª©ì˜ íŠ¹ìˆ˜ ìš”ì¸)
  2) ì‹œì¥ ë¶„ìœ„ê¸°ê°€ ì´ ì¢…ëª©ì— ë¯¸ì¹˜ëŠ” ì œí•œì  ì˜í–¥ ë˜ëŠ” ë¦¬ìŠ¤í¬

3) reasoning ë‚´ìš© êµ¬ì¡°
- expected_trendê°€ "up"ì´ë©´ì„œ market_sentimentê°€ "ë¶€ì •ì "ì¸ ê²½ìš°:
  - ì¥ê¸° í€ë”ë©˜í„¸Â·êµ¬ì¡°ì  ì„±ì¥ ìš”ì¸ â†’ ì™œ ì‹œì¥ê³¼ ë‹¬ë¦¬ ì¢‹ê²Œ ë³´ëŠ”ì§€
  - ë‹¤ë§Œ, ì „ì²´ ì‹œì¥ì´ ë¶€ì •ì ì´ë¼ ë‹¨ê¸° ë³€ë™ì„±Â·í•˜ë½ ë¦¬ìŠ¤í¬ê°€ ì¡´ì¬í•¨ì„ í•¨ê»˜ ì–¸ê¸‰
- expected_trendê°€ "down"ì´ë©´ì„œ market_sentimentê°€ "ê¸ì •ì "ì¸ ê²½ìš°:
  - ì‚°ì—… êµ¬ì¡° ë³€í™”, ê²½ìŸ ì‹¬í™”, ê·œì œ, ì¼íšŒì„± í˜¸ì¬ ì†Œë©¸ ë“±
    ì¢…ëª© ê³ ìœ ì˜ ì•…ì¬ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…
  - ì‹œì¥ì´ ì¢‹ë”ë¼ë„ ì´ ì¢…ëª©ì—ëŠ” ì™œ ì§€ì†ì ìœ¼ë¡œ ë¶ˆë¦¬í•œì§€ ì„œìˆ 

4) ì¼ê´€ì„± ê²€ì¦ ê·œì¹™
- ëª¨ë¸ì€ ê° ì¢…ëª©ë³„ë¡œ ë‹¤ìŒì„ ìŠ¤ìŠ¤ë¡œ ì ê²€í•´ì•¼ í•œë‹¤.
  - market_sentimentì™€ expected_trendê°€ ë‹¤ë¥¸ ë°©í–¥ì¼ ê²½ìš°,
    reasoning ì•ˆì— 'ì‹œì¥ vs ì¢…ëª©' ê´€ì ì˜ ì„¤ëª…ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•œë‹¤.
  - ë§Œì•½ ê·¸ëŸ° ì„¤ëª…ì´ ì—†ë‹¤ë©´, reasoningì„ ìˆ˜ì •í•˜ì—¬
    ì‹œì¥ê³¼ ì¢…ëª©ì˜ ê´€ê³„ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„¤ëª…í•œë‹¤.
"""
    
    prompt_header = f"""ì•„ë˜ëŠ” ì§€ë‚œ 24ì‹œê°„ ë™ì•ˆ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì´ë‹¤.
ê° ê¸°ì‚¬ëŠ” ë‚ ì§œ, ì œëª©, ë³¸ë¬¸, (ì¡´ì¬í•œë‹¤ë©´) ê´€ë ¨ ì¢…ëª© ì½”ë“œ/ì¢…ëª©ëª…ì„ í¬í•¨í•˜ê³  ìˆë‹¤.
ì´ ë‰´ìŠ¤ë“¤ì„ ë¶„ì„í•˜ì—¬, ìœ„ì—ì„œ ì œì‹œí•œ JSON ìŠ¤í‚¤ë§ˆì— ì •í™•íˆ ë§ëŠ” í•˜ë‚˜ì˜ JSON ê°ì²´ë¥¼ ì¶œë ¥í•˜ë¼.

[ë‰´ìŠ¤_ë°ì´í„°_ì‹œì‘]
{news_summary}
[ë‰´ìŠ¤_ë°ì´í„°_ë]

propagation_path ì¶œë ¥ ê·œì¹™ (í•„ìˆ˜):

- propagation_pathì—ëŠ” impact_chain_level ì´í•˜ì˜ ë‹¨ê³„ë§Œ í¬í•¨í•œë‹¤.
- impact_chain_level = 1 ì¸ ê²½ìš°:
  â†’ propagation_pathì—ëŠ” level 1ë§Œ í¬í•¨í•˜ê³ , level 2Â·3ì€ ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.
- impact_chain_level = 2 ì¸ ê²½ìš°:
  â†’ propagation_pathì—ëŠ” level 1, level 2ê¹Œì§€ë§Œ í¬í•¨í•˜ê³  level 3ì€ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.
- impact_chain_level = 3 ì¸ ê²½ìš°:
  â†’ propagation_pathì—ëŠ” level 1, level 2, level 3ì„ ëª¨ë‘ í¬í•¨í•  ìˆ˜ ìˆë‹¤.
- ìœ„ ê·œì¹™ì„ ì–´ê¸¸ ê²½ìš° ì¶œë ¥ì€ ë¬´íš¨ë¡œ ê°„ì£¼í•œë‹¤.

ì£¼ì˜:
- summaryëŠ” ì•½ 500~800ì ë¶„ëŸ‰ìœ¼ë¡œ ì‘ì„±í•˜ë˜, JSON êµ¬ì¡°ë¥¼ ê¹¨ì§€ ì•ŠëŠ” ê²ƒì„ ìµœìš°ì„ ìœ¼ë¡œ í•œë‹¤.
- industries ë°°ì—´ì€ ìµœì†Œ 1ê°œ ì´ìƒ í¬í•¨í•˜ë˜, ì˜ë¯¸ ìˆëŠ” ì‚°ì—…ë§Œ ë„£ëŠ”ë‹¤.
- ì¢…ëª© ì½”ë“œê°€ ë¶ˆëª…í™•í•˜ë©´ "stock_code": "" ë¡œ ë‘ê³ , reasoningì— ê·¸ ì´ìœ ë¥¼ ì ëŠ”ë‹¤.
- ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ê³ , ê·¸ ì™¸ ì–´ë–¤ í…ìŠ¤íŠ¸ë„ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.
- propagation_path ë°°ì—´ì˜ ê¸¸ì´ëŠ” impact_chain_level ê°’ê³¼ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•œë‹¤.
    - ì˜ˆ: impact_chain_level = 1 â†’ propagation_path ê¸¸ì´ = 1

ì¶”ê°€ ìš”êµ¬ì‚¬í•­:
- ê° ì¢…ëª©ì˜ reasoningì—ëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ ì„¸ ê°€ì§€ê°€ ëª¨ë‘ í¬í•¨ë˜ë„ë¡ í•˜ë¼.
  1) 1ì°¨/2ì°¨/3ì°¨ ì¤‘ ì–´ëŠ ë‹¨ê³„ì˜ ì˜í–¥ì¸ì§€ (impact_chain_levelì— 1, 2, 3ìœ¼ë¡œ í‘œì‹œ)
  2) ì˜í–¥ì´ ì „ì´ë˜ëŠ” êµ¬ì²´ì  ê²½ë¡œ(propagation_path ë°°ì—´ì— ë‹¨ê³„ë³„ë¡œ í•œêµ­ì–´ë¡œ ì„œìˆ )
  3) í•´ë‹¹ ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•œ ì‹ ë¢°ë„(confidence_score ê°’ê³¼, ì™œ ê·¸ ì •ë„ ì‹ ë¢°ë„ë¥¼ ë¶€ì—¬í–ˆëŠ”ì§€ì— ëŒ€í•œ ì„¤ëª…)

- ì˜ˆì‹œ:
  - ì—”ë¹„ë””ì•„ GPU ìˆ˜ìš” ê¸‰ì¦ ë‰´ìŠ¤ê°€ ìˆì„ ê²½ìš°,
    ì—”ë¹„ë””ì•„: impact_chain_level = 1, ì§ì ‘ ìˆ˜í˜œ.
    HBM ê³µê¸‰ì—…ì²´(SKí•˜ì´ë‹‰ìŠ¤ ë“±): impact_chain_level = 2, GPU ì—…ì²´ì˜ ë¶€í’ˆ ìˆ˜ìš” ì „ì´.
    HBM ì†Œì¬/ì¥ë¹„ ì—…ì²´: impact_chain_level = 3, ë©”ëª¨ë¦¬ íˆ¬ì í™•ëŒ€ì˜ í›„í–‰ ìˆ˜í˜œ.
  - ì´ì™€ ê°™ì´ í•œ ì‚°ì—…ì˜ ë³€í™”ê°€ ë°¸ë¥˜ì²´ì¸ ìƒì—ì„œ ì–´ë–»ê²Œ í™•ì‚°ë˜ëŠ”ì§€
    ìµœì†Œ 1ê°œ ì´ìƒì˜ êµ¬ì²´ì ì¸ ì—°ì‡„ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‘ì„±í•˜ë¼.

- ì‹ ë¢°ë„ê°€ ë‚®ì€ ê²½ìš°(confidence_score < 0.4)ì—ëŠ”
  reasoningì—ì„œ "ì‹œë‚˜ë¦¬ì˜¤ ë¶ˆí™•ì‹¤ì„± ë†’ìŒ", "ì¶”ê°€ ë°ì´í„° í•„ìš”" ë“±ìœ¼ë¡œ ëª…ì‹œí•˜ê³ ,
  ë§¤ìˆ˜/ë§¤ë„ë³´ë‹¤ëŠ” ê´€ì°°Â·ëª¨ë‹ˆí„°ë§ ëŒ€ìƒìœ¼ë¡œ ì„¤ëª…í•˜ë¼.

- ì‹œì¥ ë¶„ìœ„ê¸°(market_sentiment)ê°€ ë¶€ì •ì ì´ë”ë¼ë„, ì¼ë¶€ ì¢…ëª©ì€ êµ¬ì¡°ì ìœ¼ë¡œ ê°•í•œ ì„±ì¥ì„±ì´ë‚˜ ì‹¤ì  ê°œì„  ìš”ì¸ìœ¼ë¡œ ì¸í•´ "up" ë˜ëŠ” "ë³´ìœ " íŒë‹¨ì„ ë‚´ë¦´ ìˆ˜ ìˆë‹¤.
- ë°˜ëŒ€ë¡œ ì‹œì¥ ë¶„ìœ„ê¸°ê°€ ê¸ì •ì ì´ë”ë¼ë„, íŠ¹ì • ì¢…ëª©ì€ ê²½ìŸë ¥ ì•½í™”, ê·œì œ, ìˆ˜ìš” ê°ì†Œ ë“±ìœ¼ë¡œ ì¸í•´ "down" ë˜ëŠ” "ë§¤ë„" íŒë‹¨ì„ ë‚´ë¦´ ìˆ˜ ìˆë‹¤.
- ì´ëŸ¬í•œ ì‹œì¥ê³¼ ë°˜ëŒ€ ë°©í–¥ì˜ íŒë‹¨ì„ ë‚´ë¦¬ëŠ” ê²½ìš°,
  reasoning ì•ˆì— ë°˜ë“œì‹œ
  "ì‹œì¥ ì „ì²´ ë¶„ìœ„ê¸°"ì™€ "í•´ë‹¹ ì¢…ëª©ì˜ ê°œë³„ ìš”ì¸"ì„ ë¹„êµí•˜ì—¬ ì„¤ëª…í•˜ë¼.

- reasoning ì‘ì„± í…œí”Œë¦¿ (ë°˜ë“œì‹œ ì´ êµ¬ì¡°ë¥¼ ë”°ë¥´ë¼):
"[impact_chain_level]ì°¨ ì˜í–¥ | [propagation_path ìš”ì•½] | ì‹ ë¢°ë„[confidence_score]: [ì‹ ë¢°ë„ ê·¼ê±°]

ì¥ì : [êµ¬ì²´ì  í˜¸ì¬/ìˆ˜í˜œ ìš”ì¸]
ë¦¬ìŠ¤í¬: [ì‹œì¥/ê²½ìŸ/ê·œì œ ë¦¬ìŠ¤í¬]
ê²°ë¡ : [buy/hold/sell íŒë‹¨ + ì‹œì¥ ë¶„ìœ„ê¸° ê³ ë ¤]"
    
ì˜ˆì‹œ:
"2ì°¨ ì˜í–¥ | GPU ìˆ˜ìš”â†’HBM ìˆ˜ìš” ì „ì´ | ì‹ ë¢°ë„0.7: ì—”ë¹„ë””ì•„ ì‹¤ì œ ê³ ê°ì‚¬
 
ì¥ì : HBM ê³ ë§ˆì§„Â·ê³ ìˆ˜ìœ¨ë¡œ ì´ìµ ê°œì„  ê¸°ëŒ€
ë¦¬ìŠ¤í¬: ê¸€ë¡œë²Œ ê²½ê¸° ë‘”í™”ë¡œ ë°ì´í„°ì„¼í„° íˆ¬ì ì§€ì—° ê°€ëŠ¥ì„±  
ê²°ë¡ : ì‹œì¥ ë¶€ì •ì ì´ë‚˜ HBM êµ¬ì¡°ì  ìˆ˜ìš”ë¡œ ë³´ìœ  ì ì •"
"""

    # ì‹¤ì œ ì‘ë™ ì˜ˆì‹œëŠ” f-string ë°”ê¹¥ì˜ ì¼ë°˜ ë¬¸ìì—´ë¡œ ë¶„ë¦¬í•˜ì—¬
    # ì¤‘ê´„í˜¸({})ë¥¼ í¬í•¨í•˜ë”ë¼ë„ í¬ë§· ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•Šë„ë¡ ì²˜ë¦¬
    example_block = """

ì‹¤ì œ ì‘ë™ ì˜ˆì‹œ (íŒ¨í„´ ë³µì‚¬ ê¸ˆì§€, êµ¬ì¡°ë§Œ í•™ìŠµ):

ë‰´ìŠ¤: "ì‚¼ì„±ì „ì HBM ë§¤ì¶œ 3ë°° ì¦ê°€, AI ì„œë²„ ìˆ˜ìš” ê¸‰ì¦"

1ì°¨: ì‚¼ì„±ì „ì (000660)
json
{
    "impact_chain_level": 1,
    "propagation_path": [
        "1ì°¨: HBM ë§¤ì¶œ 3ë°° ì¦ê°€ ì§ì ‘ ì–¸ê¸‰",
        "íˆ¬ì ë…¼ë¦¬: ë°˜ë„ì²´ ë¶€ë¬¸ ì‹¤ì  ê°œì„ ",
        "ì‹ ë¢°ë„ ê·¼ê±°: ë‰´ìŠ¤ ìˆ˜ì¹˜ ì§ì ‘ ëª…ì‹œ"
    ],
    "confidence_score": 0.9,
    "reasoning": "1ì°¨ ì˜í–¥ | HBM ë§¤ì¶œ 3ë°° ì§ì ‘ í™•ì¸ | ì‹ ë¢°ë„0.9: ë‰´ìŠ¤ ìˆ˜ì¹˜ ëª…ì‹œ\\n\\nì¥ì : HBM ê³ ë§ˆì§„ìœ¼ë¡œ ë°˜ë„ì²´ ì´ìµë¥  ëŒ€í­ ê°œì„ \\në¦¬ìŠ¤í¬: ë©”ëª¨ë¦¬ ì‚¬ì´í´ í•˜ë‹¨ ê°€ëŠ¥ì„±\\nê²°ë¡ : ì‹œì¥ ë³€ë™ì„± ì¡´ì¬í•˜ë‚˜ 1ì°¨ ìˆ˜í˜œë¡œ ê°•ë ¥ ë§¤ìˆ˜"
}

2ì°¨: SKí•˜ì´ë‹‰ìŠ¤ (000660) 
json
{
    "impact_chain_level": 2,
    "propagation_path": [
        "1ì°¨: ì‚¼ì„±ì „ì HBM ë§¤ì¶œ 3ë°° ì¦ê°€",
        "2ì°¨: HBM ì‹œì¥ 1ìœ„ SKí•˜ì´ë‹‰ìŠ¤ ìˆ˜í˜œ",
        "ì‹ ë¢°ë„ ê·¼ê±°: HBM ì‹œì¥ì ìœ ìœ¨ 50% ì´ìƒ"
    ],
    "confidence_score": 0.8,
    "reasoning": "2ì°¨ ì˜í–¥ | ì‚¼ì„±ì „ì HBMâ†’ì‹œì¥ ì „ì²´ ìˆ˜ìš” í™•ëŒ€ | ì‹ ë¢°ë„0.8: ì‹œì¥ 1ìœ„\\n\\nì¥ì : HBM ì„ ë‹¨ê³µì • ê²½ìŸë ¥ìœ¼ë¡œ ì ìœ ìœ¨ í™•ëŒ€\\në¦¬ìŠ¤í¬: ê°€ê²© ê²½ìŸ ì‹¬í™” ê°€ëŠ¥ì„±\\nê²°ë¡ : ì‹œì¥ ì¡°ì •ì—ë„ êµ¬ì¡°ì  ìˆ˜ìš”ë¡œ ë³´ìœ /ì¶”ê°€ë§¤ìˆ˜"
}

3ì°¨: í›„ê³µì • ì¥ë¹„ (ì˜ˆ: í•œë¯¸ë°˜ë„ì²´)
json
{
    "impact_chain_level": 3,
    "propagation_path": [
        "1ì°¨: ì‚¼ì„±ì „ì HBM ìƒì‚° í™•ëŒ€",
        "2ì°¨: SKí•˜ì´ë‹‰ìŠ¤ HBM ìƒì‚° í™•ëŒ€",
        "3ì°¨: í›„ê³µì • ì¥ë¹„ íˆ¬ì ì¦ê°€",
        "ì‹ ë¢°ë„ ê·¼ê±°: HBM ìƒì‚° ì¦ê°€ì‹œ ì¥ë¹„ ìˆ˜ìš” ë™ë°˜ ì¦ê°€"
    ],
    "confidence_score": 0.5,
    "reasoning": "3ì°¨ ì˜í–¥ | HBM ìƒì‚°â†’ì¥ë¹„ íˆ¬ì | ì‹ ë¢°ë„0.5: ì‚¬ì´í´ ì˜ì¡´ë„\\n\\nì¥ì : HBM ìƒì‚° ì¦ê°€ì‹œ í›„ê³µì • ìˆ˜í˜œ\\në¦¬ìŠ¤í¬: íˆ¬ì ì‹œì  ë¶ˆí™•ì‹¤, ì‚¬ì´í´ ì˜ì¡´ë„ ë†’ìŒ\\nê²°ë¡ : ê´€ì°° í›„ 2ì°¨ í™•ì¸ì‹œ ì§„ì… ê³ ë ¤"
}
"""

    prompt = prompt_header + example_block

    try:
        # Gemini ëª¨ë¸ ì‚¬ìš©
        model = genai.GenerativeModel('gemini-2.5-flash')
        full_prompt = f"{system_prompt}\n\n{prompt}"
        
        response = model.generate_content(
            full_prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.3  # ê²€ì¦ì€ ë³´ìˆ˜ì ìœ¼ë¡œ
            )
        )

        result_text = response.text
        cleaned_text = result_text.strip()
        if cleaned_text.startswith("```"):
            cleaned_text = re.sub(r'^```(?:json)?\s*\n?', '', cleaned_text)
            cleaned_text = re.sub(r'\n?```\s*$', '', cleaned_text)

        try:
            parsed = json.loads(cleaned_text)
        except json.JSONDecodeError as e:
            raise ValueError(
                f"LLM JSON íŒŒì‹± ì‹¤íŒ¨\n{e}\n\nì›ë³¸ ì‘ë‹µ:\n{result_text}"
            )
        
        # í”„ë¡¬í”„íŠ¸/ìŠ¤í‚¤ë§ˆ ë³€ê²½ì—ë„ í›„ì† ë¡œì§ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ ì •ê·œí™”
        result = _normalize_analysis_result(parsed)

        # result_textë¥¼ ê²°ê³¼ì— í¬í•¨
        result["result_text"] = result_text
        
        return result
    except json.JSONDecodeError as e:
        print(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        print(f"ì‘ë‹µ í…ìŠ¤íŠ¸: {result_text if 'result_text' in locals() else 'N/A'}")
        raise ValueError(f"AI ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    except Exception as e:
        import traceback
        print(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        print(f"Traceback: {traceback.format_exc()}")
        raise


def save_analysis_to_db(
    db: Session,
    news_articles: List[NewsArticle],
    analysis_result: Dict,
    analysis_date: date
) -> Report:
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        news_articles: ë¶„ì„ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        analysis_result: AI ë¶„ì„ ê²°ê³¼
        analysis_date: ë¶„ì„ ë‚ ì§œ
    
    Returns:
        ìƒì„±ëœ Report ê°ì²´
    """
    # Report ìƒì„±
    report = Report(
        title=f"{analysis_date.strftime('%Y-%m-%d')} ì£¼ì‹ ë™í–¥ ë¶„ì„",
        summary=analysis_result.get("summary", ""),
        analysis_date=analysis_date
    )
    db.add(report)
    db.flush()  # IDë¥¼ ì–»ê¸° ìœ„í•´ flush
    
    # ë‰´ìŠ¤ ì—°ê²°
    for news in news_articles:
        report.news_articles.append(news)
    
    # ì‚°ì—… ë° ì£¼ì‹ ì €ì¥
    for industry_data in analysis_result.get("industries", []):
        # impact_descriptionì´ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        impact_desc = industry_data.get("impact_description", "")
        if isinstance(impact_desc, dict):
            impact_desc = json.dumps(impact_desc, ensure_ascii=False)
        elif not isinstance(impact_desc, str):
            impact_desc = str(impact_desc)
        
        # trend_direction ê°’ ì •ê·œí™” (up/down -> positive/negative)
        trend_direction = industry_data.get("trend_direction", "neutral")
        if trend_direction == "up":
            trend_direction = "positive"
        elif trend_direction == "down":
            trend_direction = "negative"
        
        industry = ReportIndustry(
            report_id=report.id,
            industry_name=industry_data.get("industry_name", ""),
            impact_level=industry_data.get("impact_level", "medium"),
            impact_description=impact_desc,
            trend_direction=trend_direction
        )
        db.add(industry)
        db.flush()
        
        # ì£¼ì‹ ì €ì¥
        for stock_data in industry_data.get("stocks", []):
            stock = ReportStock(
                report_id=report.id,
                industry_id=industry.id,
                stock_code=stock_data.get("stock_code", ""),
                stock_name=stock_data.get("stock_name", ""),
                expected_trend=stock_data.get("expected_trend", "neutral"),
                confidence_score=float(stock_data.get("confidence_score", 0.5)),
                reasoning=stock_data.get("reasoning", "")
            )
            db.add(stock)
    
    db.commit()
    db.refresh(report)
    
    return report


def _convert_langgraph_result_to_analysis_format(pipeline_result: Dict, news_articles: List[NewsArticle]) -> Dict:
    """
    LangGraph íŒŒì´í”„ë¼ì¸ ê²°ê³¼ë¥¼ ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    LangGraphì˜ ìµœì¢… ê²€ì¦ ê²°ê³¼ë¥¼ í¬í•¨í•˜ì—¬ ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        pipeline_result: LangGraph íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ (ìµœì¢… ê²€ì¦ ì™„ë£Œëœ ê²°ê³¼)
        news_articles: ë¶„ì„ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ê¸°ì¡´ í˜•ì‹ì˜ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (analyze_news_with_aiì™€ ë™ì¼í•œ í˜•ì‹)
        ê²€ì¦ ê²°ê³¼ë„ í¬í•¨ë¨
    """
    # LangGraph ê²°ê³¼ì—ì„œ ì‚°ì—… ë° ì£¼ì‹ ì •ë³´ ì¶”ì¶œ (ìµœì¢… ê²€ì¦ëœ ê²°ê³¼)
    primary_industry = pipeline_result.get("primary_industry", "")
    primary_reasoning = pipeline_result.get("primary_reasoning", "")
    primary_stocks = pipeline_result.get("primary_stocks", [])
    
    # ê²€ì¦ ë©”ì‹œì§€ ì¶”ì¶œ
    primary_validation_msg = pipeline_result.get("primary_validation_msg", "")
    secondary_validation_msg = pipeline_result.get("secondary_validation_msg", "")
    
    # report_payloadì—ì„œ ë” ìì„¸í•œ ì •ë³´ ì¶”ì¶œ ì‹œë„
    report_payload = pipeline_result.get("report_payload", {})
    if isinstance(report_payload, dict):
        if not primary_industry and report_payload.get("primary_industry"):
            primary_industry = report_payload.get("primary_industry")
        if not primary_reasoning and report_payload.get("primary_reasoning"):
            primary_reasoning = report_payload.get("primary_reasoning")
    
    # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (analyze_news_with_aiì™€ ë™ì¼í•œ í˜•ì‹)
    industries = []
    
    # ì£¼ì‹ ì •ë³´ ë³€í™˜
    stocks = []
    for stock in primary_stocks:
        if isinstance(stock, dict):
            stocks.append({
                "stock_code": stock.get("code", ""),
                "stock_name": stock.get("name", ""),
                "expected_trend": "up",  # ê¸°ë³¸ê°’
                "confidence_score": 0.7,  # ê¸°ë³¸ê°’
                "reasoning": primary_reasoning or f"{primary_industry} ì‚°ì—… ê´€ë ¨ ë‰´ìŠ¤ ê¸°ë°˜ ë¶„ì„"
            })
    
    # ì‚°ì—… ì •ë³´ ìƒì„±
    if primary_industry:
        industries.append({
            "industry_name": primary_industry,
            "impact_level": "high",
            "trend_direction": "positive",
            "impact_description": {
                "market_summary": {
                    "market_sentiment": "positive",
                    "key_themes": []
                },
                "buy_candidates": [{
                    "industry": primary_industry,
                    "reason_industry": primary_reasoning or f"{primary_industry} ì‚°ì—… ê´€ë ¨ ë‰´ìŠ¤ ê¸°ë°˜ ë¶„ì„",
                    "stocks": stocks
                }],
                "hold_candidates": [],
                "sell_candidates": []
            },
            "stocks": stocks
        })
    else:
        # ê¸°ë³¸ ì‚°ì—… ì •ë³´ ìƒì„±
        industries.append({
            "industry_name": "ì‹œì¥ ì¢…í•© ë° íˆ¬ì ì „ëµ",
            "impact_level": "medium",
            "trend_direction": "neutral",
            "impact_description": {
                "market_summary": {
                    "market_sentiment": "neutral",
                    "key_themes": []
                },
                "buy_candidates": [],
                "hold_candidates": [],
                "sell_candidates": []
            },
            "stocks": []
        })
    
    # ìš”ì•½ ìƒì„± (ê²€ì¦ ê²°ê³¼ í¬í•¨)
    summary = report_payload.get("report_summary") if isinstance(report_payload, dict) else None
    if not summary:
        if primary_industry:
            summary = f"LangGraph íŒŒì´í”„ë¼ì¸ ë¶„ì„ ê²°ê³¼: {primary_industry} ì‚°ì—… ì¤‘ì‹¬ ë¶„ì„. {primary_reasoning}"
        else:
            summary = "LangGraph íŒŒì´í”„ë¼ì¸ ë¶„ì„ ê²°ê³¼"
    
    # ê²€ì¦ ë©”ì‹œì§€ ì¶”ê°€
    if primary_validation_msg:
        summary += f"\n[Primary ê²€ì¦] {primary_validation_msg}"
    if secondary_validation_msg:
        summary += f"\n[Secondary ê²€ì¦] {secondary_validation_msg}"
    
    # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ì •ê·œí™” (analyze_news_with_aiì™€ ë™ì¼í•˜ê²Œ)
    analysis_result = {
        "summary": summary,
        "industries": industries
    }
    
    # ì •ê·œí™” í•¨ìˆ˜ ì ìš©
    normalized_result = _normalize_analysis_result(analysis_result)
    
    # result_text ìƒì„±
    result_text = json.dumps(normalized_result, ensure_ascii=False, indent=2)
    normalized_result["result_text"] = result_text
    
    return normalized_result


# ==================== LangGraph Pipeline (from analysis_pipeline_skeleton.py) ====================

if LANGGRAPH_AVAILABLE:
    # ==================== State ====================
    
    class PipelineState(TypedDict):
        # Input
        news_content: str
        corp_code_map: Optional[Dict[str, str]]
    
        # Node 1 outputs
        primary_industry: Optional[str]
        primary_reasoning: Optional[str]
        primary_stocks: List[Dict[str, str]]  # [{"code": "...", "name": "..."}]
    
        # Validation (Node 2)
        primary_industry_valid: bool
        primary_stocks_valid: bool
        primary_validation_msg: str
    
        # Exclusions for Node 1
        excluded_industries: List[str]
        excluded_stocks: List[str]
    
        # Financial data (Node 4 input)
        financial_statements: Optional[str]
    
        # Node 4 outputs (secondary perspective)
        secondary_industry: Optional[str]
        secondary_reasoning: Optional[str]
        secondary_stocks: List[Dict[str, str]]
    
        # Validation (Node 5)
        secondary_industry_valid: bool
        secondary_stocks_valid: bool
        secondary_validation_msg: str
    
        # Exclusions for Node 4
        excluded_secondary_industries: List[str]
        excluded_secondary_stocks: List[str]
    
        # Report (Node 7)
        report_summary: Optional[str]
        report_payload: Optional[Dict]
    
        # Loop control
        max_retries: int
        primary_retry_count: int
        secondary_retry_count: int
    
    
    # ==================== Helper Functions ====================
    
    def _build_financial_statements_from_dart(
        stocks: List[Dict[str, str]],
        corp_code_map: Optional[Dict[str, str]],
        bsns_year: str = "2023",
        reprt_code: str = "11011",
    ) -> Optional[str]:
        if not stocks or not corp_code_map:
            return None
    
        try:
            from app.stock_api.dart_api import get_financial_statements
        except ImportError:
            return None
    
        corp_codes: List[str] = []
        code_to_name: Dict[str, str] = {}
        for stock in stocks:
            stock_code = stock.get("code")
            stock_name = stock.get("name") or stock_code
            corp_code = corp_code_map.get(stock_code) if stock_code else None
            if corp_code:
                corp_codes.append(corp_code)
                code_to_name[corp_code] = stock_name
    
        if not corp_codes:
            return None
    
        result = get_financial_statements(corp_codes, bsns_year=bsns_year, reprt_code=reprt_code)
        if not result.get("success"):
            return None
    
        items = result.get("data", [])
        if not items:
            return None
    
        target_accounts = ("ìê¸°ìë³¸ë¹„ìœ¨", "ë¶€ì±„ë¹„ìœ¨", "ìœ ë™ë¹„ìœ¨")
        grouped: Dict[str, List[Dict]] = {code: [] for code in corp_codes}
        for item in items:
            corp_code = item.get("corp_code")
            account_nm = item.get("account_nm", "")
            if corp_code in grouped and any(key in account_nm for key in target_accounts):
                grouped[corp_code].append(item)
    
        lines: List[str] = []
        for corp_code in corp_codes:
            name = code_to_name.get(corp_code, corp_code)
            lines.append(f"[{name} ì¬ë¬´ì œí‘œ]")
            entries = grouped.get(corp_code) or []
            if not entries:
                lines.append(" - í•µì‹¬ ì§€í‘œ ë°ì´í„° ì—†ìŒ")
                continue
            for entry in entries:
                account_nm = entry.get("account_nm", "í•­ëª©")
                amount = entry.get("thstrm_amount", "N/A")
                currency = entry.get("currency", "")
                lines.append(f" - {account_nm}: {amount} {currency}".strip())
    
        return "\n".join(lines)
    
    
    def _extract_json_from_text(text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ JSONì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        start = text.find("{")
        end = text.rfind("}")
        if start == -1 or end == -1 or end <= start:
            return ""
        return text[start : end + 1]
    
    
    # ==================== Nodes ====================
    
    def _node_primary_recommendation(state: PipelineState) -> Dict:
        """Node 1: Analyze news -> derive industry + primary stocks."""
        system_prompt = """
ë‹¹ì‹ ì€ ì¥ê¸°íˆ¬ì ê´€ì ì˜ ì£¼ì‹ ë¦¬ì„œì¹˜ ì• ë„ë¦¬ìŠ¤íŠ¸ì´ì í¬íŠ¸í´ë¦¬ì˜¤ ë§¤ë‹ˆì €ë‹¤.

ì—­í• :
- íˆ¬ì ê¸°ê°„: ìµœì†Œ 3ë…„ ì´ìƒì˜ ì¥ê¸°íˆ¬ì.
- ìŠ¤íƒ€ì¼: ì„±ì¥ì„±ê³¼ ì¬ë¬´ ê±´ì „ì„±ì„ ì¤‘ì‹œí•˜ëŠ” Bottom-up + Top-down í˜¼í•©.
- ëª©í‘œ: ì§€ë‚œ 24ì‹œê°„ ë‰´ìŠ¤ì— ê¸°ë°˜í•´
  1) ìƒˆë¡œ ë§¤ìˆ˜í•  ìœ ë§ ì‚°ì—…êµ°ê³¼ ì¢…ëª©
  2) ê¸°ì¡´ ë³´ìœ  ì‹œ ê³„ì† ë³´ìœ í•  ìœ ë§ ì‚°ì—…êµ°ê³¼ ì¢…ëª©
  3) ë‹¨ê³„ì  ë§¤ë„ë¥¼ ê³ ë ¤í•´ì•¼ í•  ì‚°ì—…êµ°ê³¼ ì¢…ëª©
  ì„ ì‹ë³„í•˜ê³ , ê·¸ ê·¼ê±°ë¥¼ ìš”ì•½í•´ ì œì‹œí•œë‹¤.
- ì¶”ê°€ ëª©í‘œ: ë‰´ìŠ¤ì—ì„œ ë“œëŸ¬ë‚œ 1ì°¨ì (ì§ì ‘) ìˆ˜í˜œ/í”¼í•´ë¿ ì•„ë‹ˆë¼,
  ê³µê¸‰ë§, ê³ ê° ì‚°ì—…, ê²½ìŸ ì‚°ì—…, ëŒ€ì²´ì¬Â·ë³´ì™„ì¬ ê´€ì ì—ì„œ
  2ì°¨Â·3ì°¨ë¡œ íŒŒê¸‰ë˜ëŠ” ì‚°ì—…/ì¢…ëª©ê¹Œì§€ êµ¬ì¡°ì ìœ¼ë¡œ ì˜ˆì¸¡í•œë‹¤.

ì œí•œì‚¬í•­:
- ë‹¨ê¸° ë‰´ìŠ¤ ëª¨ë©˜í…€ë§Œìœ¼ë¡œ ë§¤ìˆ˜/ë§¤ë„ ê²°ì •ì„ ë‚´ë¦¬ì§€ ë§ê³ , ì¥ê¸° êµ¬ì¡°ì  ì„±ì¥ ê°€ëŠ¥ì„±ê³¼ ë¦¬ìŠ¤í¬ë¥¼ í•¨ê»˜ í‰ê°€í•˜ë¼.
- ê³¼ë„í•˜ê²Œ ê³µê²©ì ì´ê±°ë‚˜ íˆ¬ê¸°ì ì¸ í‘œí˜„(â€œë¬´ì¡°ê±´ ì˜¤ë¥¸ë‹¤â€ ë“±)ì€ ê¸ˆì§€í•œë‹¤.
- ë‰´ìŠ¤ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì‚¬ì‹¤ì„ ë‹¨ì •ì ìœ¼ë¡œ ë§Œë“¤ì–´ë‚´ì§€ ë§ê³ ,
  ì—°ì‡„ ì‹œë‚˜ë¦¬ì˜¤ë„ 'í•©ë¦¬ì ì¸ ì¶”ë¡ ' ìˆ˜ì¤€ì—ì„œë§Œ ì œì‹œí•˜ê³ ,
  ë¶ˆí™•ì‹¤ì„±ì´ í¬ë©´ reasoningì— ê·¸ ì‚¬ì‹¤ì„ ëª…ì‹œí•˜ë¼.
- ì‹ ë¢°ë„(confidence_score)ê°€ ë‚®ì€ ê²½ìš°(ì˜ˆ: 0.4 ë¯¸ë§Œ)ì—ëŠ”
  êµ¬ì²´ì ì¸ ë§¤ìˆ˜/ë§¤ë„ë³´ë‹¤ëŠ” ê´€ì°°Â·ëª¨ë‹ˆí„°ë§ ëŒ€ìƒìœ¼ë¡œ ì„œìˆ í•˜ë¼.
- reasoningì˜ ì²« ë¬¸ì¥ì€ ë°˜ë“œì‹œ impact_chain_levelê³¼ propagation_path ë²”ìœ„ë¥¼ ëª…ì‹œí•œë‹¤.
- impact_chain_levelë³´ë‹¤ ë†’ì€ ë‹¨ê³„(ì˜ˆ: 1ì°¨ ì¢…ëª©ì—ì„œ 2ì°¨Â·3ì°¨ ì˜í–¥)ëŠ”
  reasoningê³¼ propagation_path ì–´ë””ì—ë„ ì–¸ê¸‰í•˜ì§€ ì•ŠëŠ”ë‹¤.

ì—°ì‡„ ì˜í–¥ ë¶„ì„ ê¸°ì¤€ (í•„ìˆ˜):
1) 1ì°¨ ì˜í–¥ (confidence_score â‰¥ 0.7 í•„ìˆ˜)
   - ë‰´ìŠ¤ì— ì§ì ‘ ì–¸ê¸‰ëœ ì‚°ì—…/ì¢…ëª©
   - ë˜ëŠ” ë‰´ìŠ¤ì—ì„œ ë“œëŸ¬ë‚œ ì‚¬ê±´ì´ ë§¤ì¶œ/ì´ìµì— ì§ì ‘ ì—°ê²°ë˜ëŠ” ì£¼ì²´
   - ì˜ˆ: "ì‚¼ì„±ì „ì ë°˜ë„ì²´ ë§¤ì¶œ í˜¸ì¡°" â†’ ì‚¼ì„±ì „ì (1ì°¨)

2) 2ì°¨ ì˜í–¥ (confidence_score â‰¥ 0.5 í•„ìˆ˜) 
   - 1ì°¨ ì˜í–¥ì˜ í•µì‹¬ ê³µê¸‰ì—…ì²´/ê³ ê°/íŒŒíŠ¸ë„ˆ
   - ê³µê¸‰ë§ ë¹„ì¤‘ì´ í¬ê±°ë‚˜, ì—­ì‚¬ì  ìƒê´€ê´€ê³„ê°€ ëª…í™•í•œ ê²½ìš°ë§Œ
   - ì˜ˆ: ì‚¼ì„±ì „ì ë°˜ë„ì²´ í˜¸ì¡° â†’ SKí•˜ì´ë‹‰ìŠ¤ HBM (2ì°¨, ì‹¤ì œ ê³ ê°ì‚¬ì„)

3) 3ì°¨ ì˜í–¥ (confidence_score â‰¥ 0.3, ì„ íƒì )
   - 2ì°¨ ì˜í–¥ì˜ ê³µê¸‰ì—…ì²´/ê³ ê° ë˜ëŠ” ì¸í”„ë¼/ìë³¸ì¬
   - ì—­ì‚¬ì  ì‚¬ë¡€ë‚˜ ëª…í™•í•œ ê²½ì œì  ì—°ê²°ê³ ë¦¬ê°€ ìˆì„ ë•Œë§Œ
   - ì˜ˆ: HBM ìˆ˜ìš” ì¦ê°€ â†’ í¬í† ë‹‰ìŠ¤/ì†Œì¬ ì—…ì²´ (3ì°¨)

ì¶œë ¥ í•„ìˆ˜ ê·œì¹™:
- ìµœì†Œ 1ê°œì˜ 1ì°¨ + 1ê°œì˜ 2ì°¨ ì˜í–¥ì€ ë°˜ë“œì‹œ ì œì‹œí•˜ë¼
- 3ì°¨ëŠ” ì‹ ë¢°ë„ â‰¥ 0.3ì´ê³  ë…¼ë¦¬ì  ì—°ê²°ê³ ë¦¬ê°€ ëª…í™•í•  ë•Œë§Œ ì¶”ê°€
- ê° ë‹¨ê³„ë³„ confidence_scoreëŠ” ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë¶€ì—¬í•˜ë¼:
  | ë‹¨ê³„ | ìµœì†Œ ì‹ ë¢°ë„ | ë‰´ìŠ¤ ì§ì ‘ì„± | ì—­ì‚¬ì  ì‚¬ë¡€ |
  |------|-------------|-------------|-------------|
  | 1ì°¨  | â‰¥ 0.7      | ì§ì ‘ ì–¸ê¸‰  | í•„ìš” ì—†ìŒ  |
  | 2ì°¨  | â‰¥ 0.5      | ê°„ì ‘ ì–¸ê¸‰  | ìˆìœ¼ë©´ +0.1|
  | 3ì°¨  | â‰¥ 0.3      | ì¶”ë¡        | ìˆìœ¼ë©´ +0.1|


ì¶œë ¥ í˜•ì‹:
- ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ë¼.
- JSON ì´ì™¸ì˜ ì„¤ëª…, ìì—°ì–´ ë¬¸ì¥, ë§ˆí¬ë‹¤ìš´ì€ ì¶œë ¥í•˜ì§€ ë§ˆë¼.
- ì•„ë˜ ìŠ¤í‚¤ë§ˆë¥¼ ì •í™•íˆ ë”°ë¥´ë¼.

JSON ìŠ¤í‚¤ë§ˆ:
{
  "summary": "ì•„ë˜ ì‚°ì—… ë¶„ì„ì—ëŠ” ì „ì²´ ì‹œì¥ ìš”ì•½, íˆ¬ì ì „ëµ(Buy/Hold/Sell), ì—°ì‡„ ì˜í–¥ ì‹œë‚˜ë¦¬ì˜¤ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",

  "industries": [
    {
      "industry_name": "ì‹œì¥ ì¢…í•© ë° íˆ¬ì ì „ëµ",
      "impact_level": "high" | "medium" | "low",
      "trend_direction": "positive" | "negative" | "neutral",

      "impact_description": {
        "market_summary": {
          "market_sentiment": "positive" | "negative" | "neutral",
          "key_themes": ["string"]
        },

        "buy_candidates": [
          {
            "industry": "string",
            "reason_industry": "string",
            "stocks": [
              {
                "stock_code": "string",
                "stock_name": "string",
                "expected_trend": "up",
                "confidence_score": 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0,
                "impact_chain_level": 1 | 2 | 3,
                "propagation_path": [
                {
                    "level": 1,
                    "details": [
                        "1ì°¨: ë‰´ìŠ¤ ì§ì ‘ ì–¸ê¸‰ â†’ [êµ¬ì²´ì  ì‚¬ê±´]",
                        "íˆ¬ì ë…¼ë¦¬: [ë§¤ì¶œ/ì´ìµ ì˜í–¥ ê²½ë¡œ]",
                        "ì‹ ë¢°ë„ ê·¼ê±°: [ë‰´ìŠ¤ ì§ì ‘ì„±/ìˆ˜ì¹˜ ëª…ì‹œ]",
                    ]
                },
                {
                    "level": 2,
                    "details": [
                        "1ì°¨: [1ì°¨ ì‚°ì—…/ì¢…ëª©] ìˆ˜ìš”/ê³µê¸‰ ë³€í™”",
                        "2ì°¨: [ê³µê¸‰ë§ ì—°ê²°ê³ ë¦¬] â†’ ë³¸ ì¢…ëª© ì˜í–¥", 
                        "ì‹ ë¢°ë„ ê·¼ê±°: [ì—­ì‚¬ì  ì‚¬ë¡€/ë§¤ì¶œ ë¹„ì¤‘/ê³ ê°ì‚¬ ëª…ì‹œ]",
                    ]
                },
                {
                    "level": 3,
                    "details": [
                        "1ì°¨: [1ì°¨ ì‚¬ê±´]",
                        "2ì°¨: [2ì°¨ ì‚°ì—… ì˜í–¥]", 
                        "3ì°¨: [ì¸í”„ë¼/í›„í–‰ ìˆ˜í˜œ] â†’ ë³¸ ì¢…ëª©",
                        "ì‹ ë¢°ë„ ê·¼ê±°: [ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€/íˆ¬ì ì‚¬ì´í´]",
                    ]
                }
                ],
                "reasoning": "string",
                "news_drivers": ["string"],
                "risk_factors": ["string"]
              }
            ]
          }
        ],

        "hold_candidates": [
          {
            "industry": "string",
            "reason_industry": "string",
            "stocks": [
              {
                "stock_code": "string",
                "stock_name": "string",
                "expected_trend": "neutral",
                "confidence_score": 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0,
                "impact_chain_level": 1 | 2 | 3,
                "propagation_path": [
                {
                    "level": 1,
                    "details": [
                        "1ì°¨: ë‰´ìŠ¤ ì§ì ‘ ì–¸ê¸‰ â†’ [êµ¬ì²´ì  ì‚¬ê±´]",
                        "íˆ¬ì ë…¼ë¦¬: [ë§¤ì¶œ/ì´ìµ ì˜í–¥ ê²½ë¡œ]",
                        "ì‹ ë¢°ë„ ê·¼ê±°: [ë‰´ìŠ¤ ì§ì ‘ì„±/ìˆ˜ì¹˜ ëª…ì‹œ]",
                    ]
                },
                {
                    "level": 2,
                    "details": [
                        "1ì°¨: [1ì°¨ ì‚°ì—…/ì¢…ëª©] ìˆ˜ìš”/ê³µê¸‰ ë³€í™”",
                        "2ì°¨: [ê³µê¸‰ë§ ì—°ê²°ê³ ë¦¬] â†’ ë³¸ ì¢…ëª© ì˜í–¥", 
                        "ì‹ ë¢°ë„ ê·¼ê±°: [ì—­ì‚¬ì  ì‚¬ë¡€/ë§¤ì¶œ ë¹„ì¤‘/ê³ ê°ì‚¬ ëª…ì‹œ]",
                    ]
                },
                {
                    "level": 3,
                    "details": [
                        "1ì°¨: [1ì°¨ ì‚¬ê±´]",
                        "2ì°¨: [2ì°¨ ì‚°ì—… ì˜í–¥]", 
                        "3ì°¨: [ì¸í”„ë¼/í›„í–‰ ìˆ˜í˜œ] â†’ ë³¸ ì¢…ëª©",
                        "ì‹ ë¢°ë„ ê·¼ê±°: [ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€/íˆ¬ì ì‚¬ì´í´]",
                    ]
                }
                ],
                "reasoning": "string",
                "news_drivers": ["string"],
                "risk_factors": ["string"]
              }
            ]
          }
        ],

        "sell_candidates": [
          {
            "industry": "string",
            "reason_industry": "string",
            "stocks": [
              {
                "stock_code": "string",
                "stock_name": "string",
                "expected_trend": "down",
                "confidence_score": 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0,
                "impact_chain_level": 1 | 2 | 3,
                "propagation_path": [
                {
                    "level": 1,
                    "details": [
                        "1ì°¨: ë‰´ìŠ¤ ì§ì ‘ ì–¸ê¸‰ â†’ [êµ¬ì²´ì  ì‚¬ê±´]",
                        "íˆ¬ì ë…¼ë¦¬: [ë§¤ì¶œ/ì´ìµ ì˜í–¥ ê²½ë¡œ]",
                        "ì‹ ë¢°ë„ ê·¼ê±°: [ë‰´ìŠ¤ ì§ì ‘ì„±/ìˆ˜ì¹˜ ëª…ì‹œ]",
                    ]
                },
                {
                    "level": 2,
                    "details": [
                        "1ì°¨: [1ì°¨ ì‚°ì—…/ì¢…ëª©] ìˆ˜ìš”/ê³µê¸‰ ë³€í™”",
                        "2ì°¨: [ê³µê¸‰ë§ ì—°ê²°ê³ ë¦¬] â†’ ë³¸ ì¢…ëª© ì˜í–¥", 
                        "ì‹ ë¢°ë„ ê·¼ê±°: [ì—­ì‚¬ì  ì‚¬ë¡€/ë§¤ì¶œ ë¹„ì¤‘/ê³ ê°ì‚¬ ëª…ì‹œ]",
                    ]
                },
                {
                    "level": 3,
                    "details": [
                        "1ì°¨: [1ì°¨ ì‚¬ê±´]",
                        "2ì°¨: [2ì°¨ ì‚°ì—… ì˜í–¥]", 
                        "3ì°¨: [ì¸í”„ë¼/í›„í–‰ ìˆ˜í˜œ] â†’ ë³¸ ì¢…ëª©",
                        "ì‹ ë¢°ë„ ê·¼ê±°: [ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€/íˆ¬ì ì‚¬ì´í´]",
                    ]
                }
                ],
                "reasoning": "string",
                "news_drivers": ["string"],
                "risk_factors": ["string"]
              }
            ]
          }
        ]
      },

      "stocks": []
    }
  ]
}

ì‹œì¥ê³¼ ê°œë³„ ì¢…ëª©ì˜ ê´€ê³„ ê·œì¹™:

1) ì‹œì¥ ë¶„ìœ„ê¸°(market_sentiment)ì˜ ì—­í• 
- "market_sentiment"ëŠ” ì§€ìˆ˜Â·ì‹œì¥ ì „ë°˜ì˜ ìœ„í—˜ ì„ í˜¸/íšŒí”¼ ìƒí™©ì„ ë‚˜íƒ€ë‚¸ë‹¤.
- ì´ëŠ” ê°œë³„ ì¢…ëª©ì˜ ë‹¨ê¸° ìˆ˜ê¸‰ê³¼ ë°¸ë¥˜ì—ì´ì…˜ì— ì˜í–¥ì„ ì£¼ì§€ë§Œ,
  ëª¨ë“  ì¢…ëª©ì— ë™ì¼ ë°©í–¥ì˜ ê²°ë¡ ì„ ê°•ì œë¡œ ì ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.

2) ì—­í–‰ ì‚¬ë¡€ í—ˆìš© (ì¤‘ìš”)
- ì‹œì¥ì´ ë¶€ì •ì ì´ë”ë¼ë„, êµ¬ì¡°ì ìœ¼ë¡œ ì„±ì¥ì„±ì´ í¬ê±°ë‚˜
  í€ë”ë©˜í„¸ì´ ê°œì„ ë˜ëŠ” ì†Œìˆ˜ ì¢…ëª©ì€ "up" ë˜ëŠ” "ë³´ìœ /ì¶”ê°€ ë§¤ìˆ˜" íŒë‹¨ì„ ë‚´ë¦´ ìˆ˜ ìˆë‹¤.
- ë°˜ëŒ€ë¡œ ì‹œì¥ì´ ê¸ì •ì ì´ë”ë¼ë„, ê²½ìŸë ¥ ì•½í™”Â·ê·œì œÂ·ìˆ˜ìš” ê°ì†Œ ë“±ìœ¼ë¡œ
  ì¥ê¸° ì „ë§ì´ ë‚˜ìœ ì¢…ëª©ì€ "down" ë˜ëŠ” "ë§¤ë„/ë¹„ì¤‘ ì¶•ì†Œ" íŒë‹¨ì„ ë‚´ë¦´ ìˆ˜ ìˆë‹¤.
- ì´ëŸ¬í•œ 'ì‹œì¥ê³¼ ë°˜ëŒ€ ë°©í–¥' íŒë‹¨ì„ í•˜ëŠ” ê²½ìš°,
  reasoningì—ì„œ ë°˜ë“œì‹œ ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ëª¨ë‘ ì„¤ëª…í•´ì•¼ í•œë‹¤.
  1) ì‹œì¥ ì „ì²´ì™€ ë‹¤ë¥¸ ê²°ë¡ ì„ ë‚´ë¦° ì´ìœ  (ì¢…ëª©ì˜ íŠ¹ìˆ˜ ìš”ì¸)
  2) ì‹œì¥ ë¶„ìœ„ê¸°ê°€ ì´ ì¢…ëª©ì— ë¯¸ì¹˜ëŠ” ì œí•œì  ì˜í–¥ ë˜ëŠ” ë¦¬ìŠ¤í¬

3) reasoning ë‚´ìš© êµ¬ì¡°
- expected_trendê°€ "up"ì´ë©´ì„œ market_sentimentê°€ "ë¶€ì •ì "ì¸ ê²½ìš°:
  - ì¥ê¸° í€ë”ë©˜í„¸Â·êµ¬ì¡°ì  ì„±ì¥ ìš”ì¸ â†’ ì™œ ì‹œì¥ê³¼ ë‹¬ë¦¬ ì¢‹ê²Œ ë³´ëŠ”ì§€
  - ë‹¤ë§Œ, ì „ì²´ ì‹œì¥ì´ ë¶€ì •ì ì´ë¼ ë‹¨ê¸° ë³€ë™ì„±Â·í•˜ë½ ë¦¬ìŠ¤í¬ê°€ ì¡´ì¬í•¨ì„ í•¨ê»˜ ì–¸ê¸‰
- expected_trendê°€ "down"ì´ë©´ì„œ market_sentimentê°€ "ê¸ì •ì "ì¸ ê²½ìš°:
  - ì‚°ì—… êµ¬ì¡° ë³€í™”, ê²½ìŸ ì‹¬í™”, ê·œì œ, ì¼íšŒì„± í˜¸ì¬ ì†Œë©¸ ë“±
    ì¢…ëª© ê³ ìœ ì˜ ì•…ì¬ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…
  - ì‹œì¥ì´ ì¢‹ë”ë¼ë„ ì´ ì¢…ëª©ì—ëŠ” ì™œ ì§€ì†ì ìœ¼ë¡œ ë¶ˆë¦¬í•œì§€ ì„œìˆ 

4) ì¼ê´€ì„± ê²€ì¦ ê·œì¹™
- ëª¨ë¸ì€ ê° ì¢…ëª©ë³„ë¡œ ë‹¤ìŒì„ ìŠ¤ìŠ¤ë¡œ ì ê²€í•´ì•¼ í•œë‹¤.
  - market_sentimentì™€ expected_trendê°€ ë‹¤ë¥¸ ë°©í–¥ì¼ ê²½ìš°,
    reasoning ì•ˆì— 'ì‹œì¥ vs ì¢…ëª©' ê´€ì ì˜ ì„¤ëª…ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•œë‹¤.
  - ë§Œì•½ ê·¸ëŸ° ì„¤ëª…ì´ ì—†ë‹¤ë©´, reasoningì„ ìˆ˜ì •í•˜ì—¬
    ì‹œì¥ê³¼ ì¢…ëª©ì˜ ê´€ê³„ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„¤ëª…í•œë‹¤.
"""
    
        prompt_header = f"""ì•„ë˜ëŠ” ì§€ë‚œ 24ì‹œê°„ ë™ì•ˆ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì´ë‹¤.
ê° ê¸°ì‚¬ëŠ” ë‚ ì§œ, ì œëª©, ë³¸ë¬¸, (ì¡´ì¬í•œë‹¤ë©´) ê´€ë ¨ ì¢…ëª© ì½”ë“œ/ì¢…ëª©ëª…ì„ í¬í•¨í•˜ê³  ìˆë‹¤.
ì´ ë‰´ìŠ¤ë“¤ì„ ë¶„ì„í•˜ì—¬, ìœ„ì—ì„œ ì œì‹œí•œ JSON ìŠ¤í‚¤ë§ˆì— ì •í™•íˆ ë§ëŠ” í•˜ë‚˜ì˜ JSON ê°ì²´ë¥¼ ì¶œë ¥í•˜ë¼.

[ë‰´ìŠ¤_ë°ì´í„°_ì‹œì‘]
{news_summary}
[ë‰´ìŠ¤_ë°ì´í„°_ë]

propagation_path ì¶œë ¥ ê·œì¹™ (í•„ìˆ˜):

- propagation_pathì—ëŠ” impact_chain_level ì´í•˜ì˜ ë‹¨ê³„ë§Œ í¬í•¨í•œë‹¤.
- impact_chain_level = 1 ì¸ ê²½ìš°:
  â†’ propagation_pathì—ëŠ” level 1ë§Œ í¬í•¨í•˜ê³ , level 2Â·3ì€ ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.
- impact_chain_level = 2 ì¸ ê²½ìš°:
  â†’ propagation_pathì—ëŠ” level 1, level 2ê¹Œì§€ë§Œ í¬í•¨í•˜ê³  level 3ì€ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.
- impact_chain_level = 3 ì¸ ê²½ìš°:
  â†’ propagation_pathì—ëŠ” level 1, level 2, level 3ì„ ëª¨ë‘ í¬í•¨í•  ìˆ˜ ìˆë‹¤.
- ìœ„ ê·œì¹™ì„ ì–´ê¸¸ ê²½ìš° ì¶œë ¥ì€ ë¬´íš¨ë¡œ ê°„ì£¼í•œë‹¤.

ì£¼ì˜:
- summaryëŠ” ì•½ 500~800ì ë¶„ëŸ‰ìœ¼ë¡œ ì‘ì„±í•˜ë˜, JSON êµ¬ì¡°ë¥¼ ê¹¨ì§€ ì•ŠëŠ” ê²ƒì„ ìµœìš°ì„ ìœ¼ë¡œ í•œë‹¤.
- industries ë°°ì—´ì€ ìµœì†Œ 1ê°œ ì´ìƒ í¬í•¨í•˜ë˜, ì˜ë¯¸ ìˆëŠ” ì‚°ì—…ë§Œ ë„£ëŠ”ë‹¤.
- ì¢…ëª© ì½”ë“œê°€ ë¶ˆëª…í™•í•˜ë©´ "stock_code": "" ë¡œ ë‘ê³ , reasoningì— ê·¸ ì´ìœ ë¥¼ ì ëŠ”ë‹¤.
- ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ê³ , ê·¸ ì™¸ ì–´ë–¤ í…ìŠ¤íŠ¸ë„ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.
- propagation_path ë°°ì—´ì˜ ê¸¸ì´ëŠ” impact_chain_level ê°’ê³¼ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•œë‹¤.
    - ì˜ˆ: impact_chain_level = 1 â†’ propagation_path ê¸¸ì´ = 1

ì¶”ê°€ ìš”êµ¬ì‚¬í•­:
- ê° ì¢…ëª©ì˜ reasoningì—ëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ ì„¸ ê°€ì§€ê°€ ëª¨ë‘ í¬í•¨ë˜ë„ë¡ í•˜ë¼.
  1) 1ì°¨/2ì°¨/3ì°¨ ì¤‘ ì–´ëŠ ë‹¨ê³„ì˜ ì˜í–¥ì¸ì§€ (impact_chain_levelì— 1, 2, 3ìœ¼ë¡œ í‘œì‹œ)
  2) ì˜í–¥ì´ ì „ì´ë˜ëŠ” êµ¬ì²´ì  ê²½ë¡œ(propagation_path ë°°ì—´ì— ë‹¨ê³„ë³„ë¡œ í•œêµ­ì–´ë¡œ ì„œìˆ )
  3) í•´ë‹¹ ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•œ ì‹ ë¢°ë„(confidence_score ê°’ê³¼, ì™œ ê·¸ ì •ë„ ì‹ ë¢°ë„ë¥¼ ë¶€ì—¬í–ˆëŠ”ì§€ì— ëŒ€í•œ ì„¤ëª…)

- ì˜ˆì‹œ:
  - ì—”ë¹„ë””ì•„ GPU ìˆ˜ìš” ê¸‰ì¦ ë‰´ìŠ¤ê°€ ìˆì„ ê²½ìš°,
    ì—”ë¹„ë””ì•„: impact_chain_level = 1, ì§ì ‘ ìˆ˜í˜œ.
    HBM ê³µê¸‰ì—…ì²´(SKí•˜ì´ë‹‰ìŠ¤ ë“±): impact_chain_level = 2, GPU ì—…ì²´ì˜ ë¶€í’ˆ ìˆ˜ìš” ì „ì´.
    HBM ì†Œì¬/ì¥ë¹„ ì—…ì²´: impact_chain_level = 3, ë©”ëª¨ë¦¬ íˆ¬ì í™•ëŒ€ì˜ í›„í–‰ ìˆ˜í˜œ.
  - ì´ì™€ ê°™ì´ í•œ ì‚°ì—…ì˜ ë³€í™”ê°€ ë°¸ë¥˜ì²´ì¸ ìƒì—ì„œ ì–´ë–»ê²Œ í™•ì‚°ë˜ëŠ”ì§€
    ìµœì†Œ 1ê°œ ì´ìƒì˜ êµ¬ì²´ì ì¸ ì—°ì‡„ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‘ì„±í•˜ë¼.

- ì‹ ë¢°ë„ê°€ ë‚®ì€ ê²½ìš°(confidence_score < 0.4)ì—ëŠ”
  reasoningì—ì„œ "ì‹œë‚˜ë¦¬ì˜¤ ë¶ˆí™•ì‹¤ì„± ë†’ìŒ", "ì¶”ê°€ ë°ì´í„° í•„ìš”" ë“±ìœ¼ë¡œ ëª…ì‹œí•˜ê³ ,
  ë§¤ìˆ˜/ë§¤ë„ë³´ë‹¤ëŠ” ê´€ì°°Â·ëª¨ë‹ˆí„°ë§ ëŒ€ìƒìœ¼ë¡œ ì„¤ëª…í•˜ë¼.

- ì‹œì¥ ë¶„ìœ„ê¸°(market_sentiment)ê°€ ë¶€ì •ì ì´ë”ë¼ë„, ì¼ë¶€ ì¢…ëª©ì€ êµ¬ì¡°ì ìœ¼ë¡œ ê°•í•œ ì„±ì¥ì„±ì´ë‚˜ ì‹¤ì  ê°œì„  ìš”ì¸ìœ¼ë¡œ ì¸í•´ "up" ë˜ëŠ” "ë³´ìœ " íŒë‹¨ì„ ë‚´ë¦´ ìˆ˜ ìˆë‹¤.
- ë°˜ëŒ€ë¡œ ì‹œì¥ ë¶„ìœ„ê¸°ê°€ ê¸ì •ì ì´ë”ë¼ë„, íŠ¹ì • ì¢…ëª©ì€ ê²½ìŸë ¥ ì•½í™”, ê·œì œ, ìˆ˜ìš” ê°ì†Œ ë“±ìœ¼ë¡œ ì¸í•´ "down" ë˜ëŠ” "ë§¤ë„" íŒë‹¨ì„ ë‚´ë¦´ ìˆ˜ ìˆë‹¤.
- ì´ëŸ¬í•œ ì‹œì¥ê³¼ ë°˜ëŒ€ ë°©í–¥ì˜ íŒë‹¨ì„ ë‚´ë¦¬ëŠ” ê²½ìš°,
  reasoning ì•ˆì— ë°˜ë“œì‹œ
  "ì‹œì¥ ì „ì²´ ë¶„ìœ„ê¸°"ì™€ "í•´ë‹¹ ì¢…ëª©ì˜ ê°œë³„ ìš”ì¸"ì„ ë¹„êµí•˜ì—¬ ì„¤ëª…í•˜ë¼.

- reasoning ì‘ì„± í…œí”Œë¦¿ (ë°˜ë“œì‹œ ì´ êµ¬ì¡°ë¥¼ ë”°ë¥´ë¼):
"[impact_chain_level]ì°¨ ì˜í–¥ | [propagation_path ìš”ì•½] | ì‹ ë¢°ë„[confidence_score]: [ì‹ ë¢°ë„ ê·¼ê±°]

ì¥ì : [êµ¬ì²´ì  í˜¸ì¬/ìˆ˜í˜œ ìš”ì¸]
ë¦¬ìŠ¤í¬: [ì‹œì¥/ê²½ìŸ/ê·œì œ ë¦¬ìŠ¤í¬]
ê²°ë¡ : [buy/hold/sell íŒë‹¨ + ì‹œì¥ ë¶„ìœ„ê¸° ê³ ë ¤]"
    
ì˜ˆì‹œ:
"2ì°¨ ì˜í–¥ | GPU ìˆ˜ìš”â†’HBM ìˆ˜ìš” ì „ì´ | ì‹ ë¢°ë„0.7: ì—”ë¹„ë””ì•„ ì‹¤ì œ ê³ ê°ì‚¬
 
ì¥ì : HBM ê³ ë§ˆì§„Â·ê³ ìˆ˜ìœ¨ë¡œ ì´ìµ ê°œì„  ê¸°ëŒ€
ë¦¬ìŠ¤í¬: ê¸€ë¡œë²Œ ê²½ê¸° ë‘”í™”ë¡œ ë°ì´í„°ì„¼í„° íˆ¬ì ì§€ì—° ê°€ëŠ¥ì„±  
ê²°ë¡ : ì‹œì¥ ë¶€ì •ì ì´ë‚˜ HBM êµ¬ì¡°ì  ìˆ˜ìš”ë¡œ ë³´ìœ  ì ì •"
"""

    # ì‹¤ì œ ì‘ë™ ì˜ˆì‹œëŠ” f-string ë°”ê¹¥ì˜ ì¼ë°˜ ë¬¸ìì—´ë¡œ ë¶„ë¦¬í•˜ì—¬
    # ì¤‘ê´„í˜¸({})ë¥¼ í¬í•¨í•˜ë”ë¼ë„ í¬ë§· ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•Šë„ë¡ ì²˜ë¦¬
        example_block = """

ì‹¤ì œ ì‘ë™ ì˜ˆì‹œ (íŒ¨í„´ ë³µì‚¬ ê¸ˆì§€, êµ¬ì¡°ë§Œ í•™ìŠµ):

ë‰´ìŠ¤: "ì‚¼ì„±ì „ì HBM ë§¤ì¶œ 3ë°° ì¦ê°€, AI ì„œë²„ ìˆ˜ìš” ê¸‰ì¦"

1ì°¨: ì‚¼ì„±ì „ì (000660)
json
{
    "impact_chain_level": 1,
    "propagation_path": [
        "1ì°¨: HBM ë§¤ì¶œ 3ë°° ì¦ê°€ ì§ì ‘ ì–¸ê¸‰",
        "íˆ¬ì ë…¼ë¦¬: ë°˜ë„ì²´ ë¶€ë¬¸ ì‹¤ì  ê°œì„ ",
        "ì‹ ë¢°ë„ ê·¼ê±°: ë‰´ìŠ¤ ìˆ˜ì¹˜ ì§ì ‘ ëª…ì‹œ"
    ],
    "confidence_score": 0.9,
    "reasoning": "1ì°¨ ì˜í–¥ | HBM ë§¤ì¶œ 3ë°° ì§ì ‘ í™•ì¸ | ì‹ ë¢°ë„0.9: ë‰´ìŠ¤ ìˆ˜ì¹˜ ëª…ì‹œ\\n\\nì¥ì : HBM ê³ ë§ˆì§„ìœ¼ë¡œ ë°˜ë„ì²´ ì´ìµë¥  ëŒ€í­ ê°œì„ \\në¦¬ìŠ¤í¬: ë©”ëª¨ë¦¬ ì‚¬ì´í´ í•˜ë‹¨ ê°€ëŠ¥ì„±\\nê²°ë¡ : ì‹œì¥ ë³€ë™ì„± ì¡´ì¬í•˜ë‚˜ 1ì°¨ ìˆ˜í˜œë¡œ ê°•ë ¥ ë§¤ìˆ˜"
}

2ì°¨: SKí•˜ì´ë‹‰ìŠ¤ (000660) 
json
{
    "impact_chain_level": 2,
    "propagation_path": [
        "1ì°¨: ì‚¼ì„±ì „ì HBM ë§¤ì¶œ 3ë°° ì¦ê°€",
        "2ì°¨: HBM ì‹œì¥ 1ìœ„ SKí•˜ì´ë‹‰ìŠ¤ ìˆ˜í˜œ",
        "ì‹ ë¢°ë„ ê·¼ê±°: HBM ì‹œì¥ì ìœ ìœ¨ 50% ì´ìƒ"
    ],
    "confidence_score": 0.8,
    "reasoning": "2ì°¨ ì˜í–¥ | ì‚¼ì„±ì „ì HBMâ†’ì‹œì¥ ì „ì²´ ìˆ˜ìš” í™•ëŒ€ | ì‹ ë¢°ë„0.8: ì‹œì¥ 1ìœ„\\n\\nì¥ì : HBM ì„ ë‹¨ê³µì • ê²½ìŸë ¥ìœ¼ë¡œ ì ìœ ìœ¨ í™•ëŒ€\\në¦¬ìŠ¤í¬: ê°€ê²© ê²½ìŸ ì‹¬í™” ê°€ëŠ¥ì„±\\nê²°ë¡ : ì‹œì¥ ì¡°ì •ì—ë„ êµ¬ì¡°ì  ìˆ˜ìš”ë¡œ ë³´ìœ /ì¶”ê°€ë§¤ìˆ˜"
}

3ì°¨: í›„ê³µì • ì¥ë¹„ (ì˜ˆ: í•œë¯¸ë°˜ë„ì²´)
json
{
    "impact_chain_level": 3,
    "propagation_path": [
        "1ì°¨: ì‚¼ì„±ì „ì HBM ìƒì‚° í™•ëŒ€",
        "2ì°¨: SKí•˜ì´ë‹‰ìŠ¤ HBM ìƒì‚° í™•ëŒ€",
        "3ì°¨: í›„ê³µì • ì¥ë¹„ íˆ¬ì ì¦ê°€",
        "ì‹ ë¢°ë„ ê·¼ê±°: HBM ìƒì‚° ì¦ê°€ì‹œ ì¥ë¹„ ìˆ˜ìš” ë™ë°˜ ì¦ê°€"
    ],
    "confidence_score": 0.5,
    "reasoning": "3ì°¨ ì˜í–¥ | HBM ìƒì‚°â†’ì¥ë¹„ íˆ¬ì | ì‹ ë¢°ë„0.5: ì‚¬ì´í´ ì˜ì¡´ë„\\n\\nì¥ì : HBM ìƒì‚° ì¦ê°€ì‹œ í›„ê³µì • ìˆ˜í˜œ\\në¦¬ìŠ¤í¬: íˆ¬ì ì‹œì  ë¶ˆí™•ì‹¤, ì‚¬ì´í´ ì˜ì¡´ë„ ë†’ìŒ\\nê²°ë¡ : ê´€ì°° í›„ 2ì°¨ í™•ì¸ì‹œ ì§„ì… ê³ ë ¤"
}
"""

        prompt = prompt_header + example_block
    
        google_api_key = os.getenv("GOOGLE_API_KEY") or GEMINI_API_KEY
        if not google_api_key:
            print("âš ï¸ GOOGLE_API_KEY/GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {
                "primary_industry": "",
                "primary_reasoning": "",
                "primary_stocks": [],
                "primary_retry_count": state.get("primary_retry_count", 0) + 1,
            }
    
        try:
            genai.configure(api_key=google_api_key)
            model = genai.GenerativeModel("gemini-2.5-flash")
            full_prompt = f"{system_prompt}\n\n{prompt}"
            response = model.generate_content(
                full_prompt,
                generation_config=genai.types.GenerationConfig(temperature=0.0),
            )
            raw_text = response.text if hasattr(response, "text") else str(response)
    
            payload: Dict = {}
            extracted = _extract_json_from_text(raw_text)
            if extracted:
                try:
                    payload = json.loads(extracted)
                except json.JSONDecodeError:
                    payload = {}
    
            industries = payload.get("industries", []) if isinstance(payload, dict) else []
            primary_industry = ""
            primary_reasoning = ""
            primary_stocks: List[Dict[str, str]] = []
    
            if industries:
                first = industries[0] or {}
                impact_desc = (first.get("impact_description") or {}) if isinstance(first, dict) else {}
                buy_candidates = impact_desc.get("buy_candidates", []) if isinstance(impact_desc, dict) else []
                if buy_candidates:
                    candidate = buy_candidates[0] or {}
                    primary_industry = candidate.get("industry", "")
                    primary_reasoning = candidate.get("reason_industry", "")
                    stocks = candidate.get("stocks", []) if isinstance(candidate, dict) else []
                    for stock in stocks:
                        if not isinstance(stock, dict):
                            continue
                        code = stock.get("stock_code") or stock.get("code") or ""
                        name = stock.get("stock_name") or stock.get("name") or ""
                        if code or name:
                            primary_stocks.append({"code": code, "name": name})
    
            return {
                "primary_industry": primary_industry,
                "primary_reasoning": primary_reasoning,
                "primary_stocks": primary_stocks,
                "primary_retry_count": state.get("primary_retry_count", 0) + 1,
            }
        except Exception as e:
            import traceback
            print(f"âš ï¸ Primary ì˜ˆì¸¡ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            print(f"Traceback: {traceback.format_exc()}")
            return {
                "primary_industry": "",
                "primary_reasoning": "",
                "primary_stocks": [],
                "primary_retry_count": state.get("primary_retry_count", 0) + 1,
            }
    
    
    def _node_primary_validation(state: PipelineState) -> Dict:
        """Node 2: Validate primary industry relevance and stock financial health."""
        prediction_output = {
            "industry": state.get("primary_industry", ""),
            "stocks": state.get("primary_stocks", []),
            "reasoning": state.get("primary_reasoning", ""),
        }
        original_news = state.get("news_content", "")
        financial_data = state.get("financial_statements", "")
        if not financial_data:
            financial_data = _build_financial_statements_from_dart(
                stocks=state.get("primary_stocks", []),
                corp_code_map=state.get("corp_code_map"),
            ) or ""
    
        # analysis_copy2.pyì˜ validate_prediction_with_aiì™€ ë™ì¼í•œ ê²€ì¦ ë¡œì§ ì‚¬ìš©
        system_prompt = """
ë‹¹ì‹ ì€ ì¥ê¸°íˆ¬ì ê´€ì ì˜ ì£¼ì‹ ë¶„ì„ ê²€ì¦ ì „ë¬¸ê°€ë‹¤. ì˜ˆì¸¡ LLMì˜ ì‚°ì—…/ì¢…ëª© ì¶”ì²œì„ ë‰´ìŠ¤ì™€ ì¬ë¬´ì œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì—„ê²©íˆ ê²€ì¦í•˜ë¼.
## ê²€ì¦ ì—­í•  (í•„ìˆ˜ 2ë‹¨ê³„ ìˆœì°¨ ìˆ˜í–‰)
### 1ë‹¨ê³„: ë‰´ìŠ¤-ì‚°ì—…/ì¢…ëª© ì¼ì¹˜ì„± ê²€ì¦ (ì˜ˆì¸¡ LLM ì¶œë ¥ vs ì›ë³¸ ë‰´ìŠ¤)
**ëª©í‘œ**: ì˜ˆì¸¡ LLMì´ ë‰´ìŠ¤ íë¦„ì„ ì™œê³¡/ê³¼ì¥í•˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
**ê²€ì¦ ê¸°ì¤€**:
[OK] ë‰´ìŠ¤ ì§ì ‘ ì–¸ê¸‰ or ëª…í™•í•œ 1ì°¨ ì˜í–¥ (confidence â‰¥0.7)
[OK] ë…¼ë¦¬ì  2ì°¨ ì˜í–¥ (ê³µê¸‰ë§/ê³ ê° ì—°ê²° ëª…í™•, confidence â‰¥0.5)
[X] ë‰´ìŠ¤ì™€ ë¬´ê´€í•œ ì¢…ëª© (ì¶”ë¡  ê³¼ë„)
[X] ë‰´ìŠ¤ ë°©í–¥ê³¼ ë°˜ëŒ€ ì¶”ì²œ (ì˜ˆ: ë¶€ì • ë‰´ìŠ¤â†’up ì¶”ì²œ)
[X] ê³¼ë„í•œ 3ì°¨ ì˜í–¥ (ì—°ê²°ê³ ë¦¬ í¬ë°•)
**ì¶œë ¥**: ë¶ˆì¼ì¹˜ ì‚°ì—…/ì¢…ëª© ëª©ë¡
[ë³´ìˆ˜ ì›ì¹™]
- ë‰´ìŠ¤ì™€ ì‚°ì—…/ì¢…ëª©ì˜ ì—°ê²°ì´ ì–µì§€ìŠ¤ëŸ½ê±°ë‚˜,
  ë‹¨ìˆœ í…Œë§ˆ ì—°ìƒì— ë¶ˆê³¼í•˜ë‹¤ê³  íŒë‹¨ë˜ë©´
  confidenceê°€ ë†’ë”ë¼ë„ ê°€ì°¨ ì—†ì´ news_mismatchë¡œ ë¶„ë¥˜í•œë‹¤.
- ê²€ì¦ LLMì€ ì˜ˆì¸¡ LLMì˜ ë‚™ê´€ì  í•´ì„ì„ êµì •í•˜ëŠ” ì—­í• ì„ì„ ëª…ì‹¬í•œë‹¤.
### 2ë‹¨ê³„: ì¬ë¬´ ê±´ì „ì„± ê²€ì¦ (ì¶”ì²œ ì¢…ëª© ëŒ€ìƒ)
ìœ ë™ë¹„ìœ¨ = ìœ ë™ìì‚° / ìœ ë™ë¶€ì±„
ë¶€ì±„ë¹„ìœ¨ = ë¶€ì±„ì´ê³„ / ìë³¸ì´ê³„
ìê¸°ìë³¸ë¹„ìœ¨ = ìë³¸ì´ê³„ / ìì‚°ì´ê³„
**ìš°ì„ ìˆœìœ„ ìˆœ ì ìš©** (ìê¸°ìë³¸ë¹„ìœ¨ â†’ ë¶€ì±„ë¹„ìœ¨ â†’ ìœ ë™ë¹„ìœ¨):
ìê¸°ìë³¸ë¹„ìœ¨ <30%: [ìœ„í—˜] ê²½ê¸° ì¶©ê²© ì·¨ì•½ â†’ ë§¤ìˆ˜/ë³´ìœ  ë¶€ì í•©
ë¶€ì±„ë¹„ìœ¨ >200%: [ìœ„í—˜] ì¬ë¬´êµ¬ì¡° ì·¨ì•½ â†’ ì¥ê¸°íˆ¬ì ë¦¬ìŠ¤í¬
ìœ ë™ë¹„ìœ¨ <1.0: [ìœ„í—˜] ë‹¨ê¸° ìœ ë™ì„± ìœ„ê¸° ê°€ëŠ¥ì„±
**ê±´ì „ì„± ë“±ê¸‰**:
- A: ëª¨ë“  ì§€í‘œ ì–‘í˜¸ (ìê¸°ìë³¸â‰¥30%, ë¶€ì±„â‰¤200%, ìœ ë™â‰¥1.5)
- B: 1ê°œ ì§€í‘œ ê²½ê³„ (ë³´ìˆ˜ì  ê´€ì°°)
- C: 2ê°œ ì§€í‘œ ìœ„í—˜ (ë³´ìœ  ê²€í† )
- D: 1ê°œ ì§€í‘œ ì‹¬ê° (ë§¤ë„ ê²€í† ) (ìê¸°ìë³¸<30% OR ë¶€ì±„>200% OR ìœ ë™<1.0)
- F: 2ê°œ ì´ìƒ ì‹¬ê° (ë§¤ìˆ˜ ê¸ˆì§€) (ìê¸°ìë³¸<25% OR ë¶€ì±„>250% OR ìœ ë™<0.8)
## ì¶œë ¥ ì œí•œ: ì í•©í•˜ì§€ ì•Šì€ ê²ƒë§Œ ì„ ì •
- ë‰´ìŠ¤ ì¼ì¹˜ì„± ì™„ë²½í•˜ê³  ì¬ë¬´ A/Bë“±ê¸‰ â†’ ë¹ˆ ë°°ì—´ []
- **ì„ ì • ì´ìœ  í•„ìˆ˜**: ì™œ ì´ ì‚°ì—…/ì¢…ëª©ì´ ë¶€ì í•©í•œì§€ êµ¬ì²´ì  ê·¼ê±°
## ê²€ì¦ ì¶œë ¥ í˜•ì‹ (ìœ íš¨ JSONë§Œ ì¶œë ¥)
{
  "validation_summary": "ê²€ì¦ ê²°ê³¼ ìš”ì•½: ë‰´ìŠ¤ ë¶ˆì¼ì¹˜ Xê°œ, ì¬ë¬´ìœ„í—˜ Yê°œ ì¢…ëª© ì‹ë³„ë¨.",
  "news_mismatch": [
    {
      "industry": "ì˜ˆì¸¡ ì‚°ì—…ëª…",
      "stocks": ["ì¢…ëª©ì½”ë“œ1", "ì¢…ëª©ì½”ë“œ2"],
      "mismatch_reason": "êµ¬ì²´ì  ë¶ˆì¼ì¹˜ ì‚¬ìœ  (ë‰´ìŠ¤ ì§ì ‘ì„± ë¶€ì¡±/ë°©í–¥ ë°˜ëŒ€ ë“±)",
      "evidence": "ì›ë³¸ ë‰´ìŠ¤ì—ì„œ í™•ì¸ëœ ì‚¬ì‹¤",
      "confidence_score": 0.7
    }
  ],
  "financial_risks": [
    {
      "stock_code": "ì¢…ëª©ì½”ë“œ",
      "stock_name": "ì¢…ëª©ëª…",
      "financial_metrics": {
        "self_equity_ratio": "XX%",
        "debt_ratio": "XXX%",
        "current_ratio": "X.X"
      },
      "health_grade": "A|B|C|D|F",
      "risk_priority": "ìê¸°ìë³¸|ë¶€ì±„|ìœ ë™",
      "recommendation": "ë§¤ìˆ˜ê¸ˆì§€|ë³´ìœ ê²€í† |ê´€ì°°",
      "prediction_category": "buy|hold|sell"
    }
  ],
  "overall_assessment": {
    "news_accuracy": "high|medium|low",
    "financial_soundness": "high|medium|low",
    "total_reliable_stocks": 5,
    "total_risky_stocks": 3,
    "action_required": "ì¦‰ì‹œ ìˆ˜ì •|ê´€ì°°|ì–‘í˜¸"
  }
}
""".strip()
    
        prompt = f"""
[ì˜ˆì¸¡_LLM_ì¶œë ¥]
{json.dumps(prediction_output, ensure_ascii=False, indent=2)}
[ì˜ˆì¸¡_LLM_ì¶œë ¥_ë]
[ì›ë³¸_ë‰´ìŠ¤]
{original_news}
[ì›ë³¸_ë‰´ìŠ¤_ë]
[ì¬ë¬´ì œí‘œ_ë°ì´í„°]
{financial_data}
[ì¬ë¬´ì œí‘œ_ë°ì´í„°_ë]
## ê²€ì¦ ì›ì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜)
### ë‰´ìŠ¤ ë¶ˆì¼ì¹˜ íŒì • ê¸°ì¤€
1. **ì§ì ‘ì„± ë¶€ì¡±**: ë‰´ìŠ¤ì— ì „í˜€ ì–¸ê¸‰ì—†ëŠ”ë° 1ì°¨ ì˜í–¥ ì£¼ì¥ [X]
2. **ë°©í–¥ ë°˜ëŒ€**: ë¶€ì • ë‰´ìŠ¤ì¸ë° up/confidenceâ‰¥0.7 [X]
3. **ê³¼ë„ ì¶”ë¡ **: 3ì°¨ ì˜í–¥ì— confidenceâ‰¥0.5 [X]
4. **ì‚¬ì‹¤ ì™œê³¡**: ë‰´ìŠ¤ ìˆ˜ì¹˜/ì‚¬ê±´ê³¼ ë‹¤ë¥¸ í•´ì„ [X]
### ì¬ë¬´ ìœ„í—˜ íŒì • ê¸°ì¤€ (ìš°ì„ ìˆœìœ„ ì—„ìˆ˜)
CRITICAL (Fë“±ê¸‰):
ìê¸°ìë³¸ë¹„ìœ¨ <25% OR ë¶€ì±„ë¹„ìœ¨ >250% OR ìœ ë™ë¹„ìœ¨ <0.8
HIGH RISK (Dë“±ê¸‰):
ìê¸°ìë³¸ë¹„ìœ¨ <30% OR ë¶€ì±„ë¹„ìœ¨ >200% OR ìœ ë™ë¹„ìœ¨ <1.0
MONITOR (Cë“±ê¸‰):
ìê¸°ìë³¸ë¹„ìœ¨ 30~35% OR ë¶€ì±„ë¹„ìœ¨ 150~200% OR ìœ ë™ë¹„ìœ¨ 1.0~1.2
## ì¶œë ¥ ì œí•œì‚¬í•­
- news_mismatch: ì‹¤ì œ ë¶ˆì¼ì¹˜ë§Œ (ì˜ˆì¸¡ì´ ì •í™•í•˜ë©´ ë¹ˆ ë°°ì—´ [])
- financial_risks: C/D/Fë“±ê¸‰ë§Œ (A/BëŠ” ì–‘í˜¸ë¡œ ê°„ì£¼)
- confidence_score: 0.1ë‹¨ìœ„, ë‰´ìŠ¤ ì§ì ‘ì„±ì— ë”°ë¼ 0.3~1.0
- reasoning ìƒëµ: JSON êµ¬ì¡° ì—„ìˆ˜, ìì—°ì–´ ì„¤ëª… ê¸ˆì§€
ìœ íš¨í•œ JSONë§Œ ì¶œë ¥. ë‹¤ë¥¸ ì–´ë–¤ í…ìŠ¤íŠ¸ë„ ì¶œë ¥í•˜ì§€ ë§ˆë¼.
"""
    
        google_api_key = os.getenv("GOOGLE_API_KEY") or GEMINI_API_KEY
        if not google_api_key:
            print("âš ï¸ GOOGLE_API_KEY/GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {
                "primary_industry_valid": True,
                "primary_stocks_valid": True,
                "primary_validation_msg": "API í‚¤ ì—†ìŒìœ¼ë¡œ ê²€ì¦ ê±´ë„ˆëœ€",
            }
    
        try:
            genai.configure(api_key=google_api_key)
            model = genai.GenerativeModel("gemini-2.5-flash")
            full_prompt = f"{system_prompt}\n\n{prompt}"
            response = model.generate_content(
                full_prompt,
                generation_config=genai.types.GenerationConfig(temperature=0.3),
            )
            raw_text = response.text if hasattr(response, "text") else str(response)
    
            # JSON ì¶”ì¶œ ë° íŒŒì‹±
            cleaned_text = raw_text.strip()
            if cleaned_text.startswith("```"):
                cleaned_text = re.sub(r'^```(?:json)?\s*\n?', '', cleaned_text)
                cleaned_text = re.sub(r'\n?```\s*$', '', cleaned_text)
    
            extracted = _extract_json_from_text(cleaned_text)
            if not extracted:
                print("âš ï¸ ê²€ì¦ LLM ì‘ë‹µì—ì„œ JSONì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return {
                    "primary_industry_valid": True,
                    "primary_stocks_valid": True,
                    "primary_validation_msg": "JSON ì¶”ì¶œ ì‹¤íŒ¨",
                }
    
            validation_result = json.loads(extracted)
    
            # ê²€ì¦ ê²°ê³¼ íŒŒì‹±
            news_mismatch = validation_result.get("news_mismatch", [])
            financial_risks = validation_result.get("financial_risks", [])
            overall_assessment = validation_result.get("overall_assessment", {})
    
            # ê²€ì¦ í†µê³¼ ì—¬ë¶€ íŒë‹¨
            industry_valid = len(news_mismatch) == 0
            stocks_valid = len(financial_risks) == 0 or overall_assessment.get("action_required") == "ì–‘í˜¸"
    
            # ê²€ì¦ ë©”ì‹œì§€ ìƒì„±
            validation_msg = validation_result.get("validation_summary", "")
            if not validation_msg:
                if not industry_valid:
                    validation_msg += f"ë‰´ìŠ¤ ë¶ˆì¼ì¹˜ {len(news_mismatch)}ê°œ ë°œê²¬. "
                if not stocks_valid:
                    validation_msg += f"ì¬ë¬´ ìœ„í—˜ {len(financial_risks)}ê°œ ë°œê²¬."
    
            return {
                "primary_industry_valid": industry_valid,
                "primary_stocks_valid": stocks_valid,
                "primary_validation_msg": validation_msg or "ê²€ì¦ ì™„ë£Œ",
            }
        except Exception as e:
            import traceback
            print(f"âš ï¸ ê²€ì¦ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            print(f"Traceback: {traceback.format_exc()}")
            # ê²€ì¦ ì‹¤íŒ¨ ì‹œ í†µê³¼ë¡œ ì²˜ë¦¬ (ì•ˆì „ì¥ì¹˜)
            return {
                "primary_industry_valid": True,
                "primary_stocks_valid": True,
                "primary_validation_msg": f"ê²€ì¦ ì˜¤ë¥˜: {str(e)}",
            }
    
    
    def _node_secondary_recommendation(state: PipelineState) -> Dict:
        """Node 4: Using financials + Node1 reasoning, propose affected stocks from different perspective."""
        # TODO: Secondary recommendation êµ¬í˜„ (í˜„ì¬ëŠ” ê¸°ë³¸ê°’ ë°˜í™˜)
        return {
            "secondary_industry": state.get("primary_industry", ""),
            "secondary_reasoning": state.get("primary_reasoning", ""),
            "secondary_stocks": state.get("primary_stocks", []),
            "secondary_retry_count": state.get("secondary_retry_count", 0) + 1,
        }
    
    
    def _node_secondary_validation(state: PipelineState) -> Dict:
        """Node 5: Validate secondary industry relevance and stock financial health."""
        # Primary validationê³¼ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©
        prediction_output = {
            "industry": state.get("secondary_industry", ""),
            "stocks": state.get("secondary_stocks", []),
            "reasoning": state.get("secondary_reasoning", ""),
        }
        original_news = state.get("news_content", "")
        financial_data = state.get("financial_statements", "")
        if not financial_data:
            financial_data = _build_financial_statements_from_dart(
                stocks=state.get("secondary_stocks", []),
                corp_code_map=state.get("corp_code_map"),
            ) or ""
    
        # Primary validationê³¼ ë™ì¼í•œ ê²€ì¦ ë¡œì§ (ì½”ë“œ ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ í•¨ìˆ˜ë¡œ ë¶„ë¦¬ ê°€ëŠ¥)
        # ê°„ë‹¨íˆ í†µê³¼ ì²˜ë¦¬ (í•„ìš”ì‹œ primaryì™€ ë™ì¼í•œ ë¡œì§ êµ¬í˜„)
        return {
            "secondary_industry_valid": True,
            "secondary_stocks_valid": True,
            "secondary_validation_msg": "ê²€ì¦ ì™„ë£Œ",
        }
    
    
    def _node_report_builder(state: PipelineState) -> Dict:
        """Node 7: Build report payload for UI sidebar."""
        return {
            "report_summary": f"Primary: {state.get('primary_industry', '')} / Secondary: {state.get('secondary_industry', '')}",
            "report_payload": {
                "primary_industry": state.get("primary_industry"),
                "primary_reasoning": state.get("primary_reasoning"),
                "primary_stocks": state.get("primary_stocks", []),
                "secondary_industry": state.get("secondary_industry"),
                "secondary_reasoning": state.get("secondary_reasoning"),
                "secondary_stocks": state.get("secondary_stocks", []),
            },
        }
    
    
    # ==================== Routers ====================
    
    def _route_after_primary_validation(state: PipelineState) -> str:
        """Primary validation routing: Valid -> Node 4, Invalid -> back to Node 1"""
        max_retries = state.get("max_retries", 3)
        retry_count = state.get("primary_retry_count", 0)
    
        if retry_count >= max_retries:
            print(f"âš ï¸ Primary ì˜ˆì¸¡ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries}) ë„ë‹¬. ê²€ì¦ì„ í†µê³¼ì‹œí‚µë‹ˆë‹¤.")
            return "valid"
    
        if state.get("primary_industry_valid", False) and state.get("primary_stocks_valid", False):
            return "valid"
    
        print(f"âš ï¸ Primary ì˜ˆì¸¡ ê²€ì¦ ì‹¤íŒ¨. ì¬ì‹œë„ {retry_count + 1}/{max_retries}")
        return "invalid"
    
    
    def _route_after_secondary_validation(state: PipelineState) -> str:
        """Secondary validation routing: Valid -> report, Invalid -> retry"""
        max_retries = state.get("max_retries", 3)
        retry_count = state.get("secondary_retry_count", 0)
    
        if retry_count >= max_retries:
            print(f"âš ï¸ Secondary ì˜ˆì¸¡ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries}) ë„ë‹¬. ë³´ê³ ì„œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            return "report"
    
        if not state.get("secondary_industry_valid", False):
            print(f"âš ï¸ Secondary ì˜ˆì¸¡ ê²€ì¦ ì‹¤íŒ¨ (ì‚°ì—…). ì¬ì‹œë„ {retry_count + 1}/{max_retries}")
            return "retry"
        if not state.get("secondary_stocks_valid", False):
            print(f"âš ï¸ Secondary ì˜ˆì¸¡ ê²€ì¦ ì‹¤íŒ¨ (ì£¼ì‹). ì¬ì‹œë„ {retry_count + 1}/{max_retries}")
            return "retry"
        return "report"
    
    
    # ==================== Workflow ====================
    
    def _build_pipeline():
        graph = StateGraph(PipelineState)
    
        graph.add_node("primary_recommendation", _node_primary_recommendation)
        graph.add_node("primary_validation", _node_primary_validation)
        graph.add_node("secondary_recommendation", _node_secondary_recommendation)
        graph.add_node("secondary_validation", _node_secondary_validation)
        graph.add_node("report", _node_report_builder)
    
        graph.set_entry_point("primary_recommendation")
        graph.add_edge("primary_recommendation", "primary_validation")
    
        graph.add_conditional_edges(
            "primary_validation",
            _route_after_primary_validation,
            {
                "valid": "secondary_recommendation",
                "invalid": "primary_recommendation",
            },
        )
    
        graph.add_edge("secondary_recommendation", "secondary_validation")
    
        graph.add_conditional_edges(
            "secondary_validation",
            _route_after_secondary_validation,
            {
                "retry": "secondary_recommendation",
                "report": "report",
            },
        )
    
        graph.add_edge("report", END)
    
        return graph.compile()


def run_langgraph_pipeline(news_articles: List[NewsArticle], corp_code_map: Optional[Dict[str, str]] = None) -> Dict:
    """
    LangGraph íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ì—¬ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        news_articles: ë¶„ì„í•  ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        corp_code_map: ì¢…ëª©ì½”ë“œ-ë²•ì¸ì½”ë“œ ë§¤í•‘ (ì„ íƒ)
    
    Returns:
        LangGraph íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼
    """
    if not LANGGRAPH_AVAILABLE:
        raise ValueError("LangGraph íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ë‰´ìŠ¤ ë‚´ìš© ìƒì„±
    news_items = []
    for idx, article in enumerate(news_articles[:20], 1):
        url = article.url or "URL ì—†ìŒ"
        published_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
        
        if article.article_metadata:
            metadata = article.article_metadata
            if isinstance(metadata, dict):
                url = metadata.get("url", article.url) or "URL ì—†ìŒ"
                published_date = metadata.get("published_date", "ë‚ ì§œ ì •ë³´ ì—†ìŒ")
        
        if article.published_at:
            published_date = article.published_at.strftime("%Y-%m-%d %H:%M:%S")
        
        content_preview = article.content[:500] if article.content else "ë‚´ìš© ì—†ìŒ"
        news_items.append(f"{idx}. ì œëª©: {article.title}\n   URL: {url}\n   ë°œí–‰ì¼: {published_date}\n   ë‚´ìš©: {content_preview}")
    
    news_content = "\n\n".join(news_items)
    
    # íŒŒì´í”„ë¼ì¸ ë¹Œë“œ ë° ì‹¤í–‰
    if not LANGGRAPH_AVAILABLE:
        raise ValueError("LangGraph íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    pipeline = _build_pipeline()
    
    initial_state: PipelineState = {
        "news_content": news_content,
        "corp_code_map": corp_code_map,
        "primary_industry": None,
        "primary_reasoning": None,
        "primary_stocks": [],
        "primary_industry_valid": False,
        "primary_stocks_valid": False,
        "primary_validation_msg": "",
        "excluded_industries": [],
        "excluded_stocks": [],
        "financial_statements": None,
        "secondary_industry": None,
        "secondary_reasoning": None,
        "secondary_stocks": [],
        "secondary_industry_valid": False,
        "secondary_stocks_valid": False,
        "secondary_validation_msg": "",
        "excluded_secondary_industries": [],
        "excluded_secondary_stocks": [],
        "report_summary": None,
        "report_payload": None,
        "max_retries": 3,
        "primary_retry_count": 0,
        "secondary_retry_count": 0,
    }
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    result = pipeline.invoke(initial_state)
    
    return result


def analyze_and_save(
    db: Session,
    news_articles: List[NewsArticle],
    analysis_date: Optional[date] = None
) -> Tuple[Report, str]:
    """
    ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    LangGraph íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        news_articles: ë¶„ì„í•  ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        analysis_date: ë¶„ì„ ë‚ ì§œ (ê¸°ë³¸ê°’: ì˜¤ëŠ˜)
    
    Returns:
        (ìƒì„±ëœ Report ê°ì²´, LLMì´ ìƒì„±í•œ ì›ë³¸ result_text) íŠœí”Œ
    """
    if not news_articles:
        raise ValueError("ë¶„ì„í•  ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    if analysis_date is None:
        analysis_date = date.today()
    
    # LangGraph íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
    if LANGGRAPH_AVAILABLE:
        try:
            print("ğŸ”„ LangGraph íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë¶„ì„ ì‹œì‘...")
            pipeline_result = run_langgraph_pipeline(news_articles, corp_code_map=None)
            
            # LangGraph ê²°ê³¼ë¥¼ ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            analysis_result = _convert_langgraph_result_to_analysis_format(pipeline_result, news_articles)
            
            # result_text ì¶”ì¶œ
            result_text = analysis_result.get("result_text", "")
            
            # ê²°ê³¼ ì €ì¥
            report = save_analysis_to_db(db, news_articles, analysis_result, analysis_date)
            
            print(f"âœ… LangGraph íŒŒì´í”„ë¼ì¸ ë¶„ì„ ì™„ë£Œ: ë³´ê³ ì„œ ID={report.id}")
            return report, result_text
        except Exception as e:
            import traceback
            print(f"âš ï¸  LangGraph íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            print(f"Traceback: {traceback.format_exc()}")
            print("ğŸ”„ ê¸°ë³¸ ë¶„ì„ ë°©ì‹ìœ¼ë¡œ ì „í™˜...")
            # ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ í´ë°±
    else:
        result_text = "langgraph ì‚¬ìš© ë¶ˆê°€"
        report = "langgraph ì‚¬ìš© ë¶ˆê°€"
    # # ê¸°ë³¸ ë¶„ì„ ë°©ì‹ (LangGraph ì‚¬ìš© ë¶ˆê°€ ì‹œ)
    # print("ğŸ”„ ê¸°ë³¸ ë¶„ì„ ë°©ì‹ìœ¼ë¡œ ë¶„ì„ ì‹œì‘...")
    # analysis_result = analyze_news_with_ai(news_articles)
    
    # # result_text ì¶”ì¶œ
    # result_text = analysis_result.get("result_text", "")
    
    # # ê²°ê³¼ ì €ì¥
    # report = save_analysis_to_db(db, news_articles, analysis_result, analysis_date)
    
    return report, result_text


def analyze_news_from_vector_db(
    db: Session,
    start_datetime: Optional[datetime] = None,
    end_datetime: Optional[datetime] = None,
    analysis_date: Optional[date] = None
) -> Tuple[Report, str]:
    """
    ë²¡í„° DBì—ì„œ ë‚ ì§œ ë²”ìœ„ë¡œ ë‰´ìŠ¤ë¥¼ ì¡°íšŒí•˜ê³ , AI ë¶„ì„ì„ ìˆ˜í–‰í•˜ì—¬ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ í”„ë¡¬í”„íŠ¸ì™€ ê´€ë ¨ì„±ì´ ë†’ì€ ë‰´ìŠ¤ë§Œ ì„ íƒí•©ë‹ˆë‹¤.
    ì „ì²´ í”Œë¡œìš°ë¥¼ í•˜ë‚˜ì˜ í•¨ìˆ˜ë¡œ í†µí•©í•©ë‹ˆë‹¤.
    
    Args:
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        start_datetime: ì‹œì‘ ë‚ ì§œ/ì‹œê°„ (ê¸°ë³¸ê°’: ì „ë‚  06:00:00)
        end_datetime: ì¢…ë£Œ ë‚ ì§œ/ì‹œê°„ (ê¸°ë³¸ê°’: í˜„ì¬ ì‹œê°„)
        analysis_date: ë¶„ì„ ë‚ ì§œ (ê¸°ë³¸ê°’: ì˜¤ëŠ˜)
    
    Returns:
        (ìƒì„±ëœ Report ê°ì²´, LLMì´ ìƒì„±í•œ ì›ë³¸ result_text) íŠœí”Œ
    
    Raises:
        ValueError: ë‰´ìŠ¤ê°€ ì—†ê±°ë‚˜ ë¶„ì„ ì‹¤íŒ¨ ì‹œ
    """
    # ë¶„ì„ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    # ì£¼ì‹ ì‹œì¥ ë™í–¥ ë¶„ì„ì— ê´€ë ¨ëœ í‚¤ì›Œë“œë¡œ ì¿¼ë¦¬ ìƒì„±
    query_text = "ì£¼ì‹ ì‹œì¥ ë™í–¥ ë¶„ì„ ì‚°ì—…ë³„ ì˜í–¥ ì£¼ì‹ ì˜ˆì¸¡"
    query_embedding = create_query_embedding(query_text)
    
    # ë²¡í„° DBì—ì„œ ë‰´ìŠ¤ ì¡°íšŒ (ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ì‚¬ìš©)
    if query_embedding:
        news_articles = get_news_by_date_range(
            db=db,
            start_datetime=start_datetime,
            end_datetime=end_datetime,
            query_embedding=query_embedding,
            limit=20  # ìƒìœ„ 20ê°œë§Œ ì„ íƒ
        )
        print(f"âœ… ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ {len(news_articles)}ê°œ ë‰´ìŠ¤ ì„ íƒ")
    else:
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        print("âš ï¸  ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨, ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ë§Œ ì‚¬ìš©")
        news_articles = get_news_by_date_range(
            db=db,
            start_datetime=start_datetime,
            end_datetime=end_datetime,
            limit=20
        )
    
    if not news_articles:
        raise ValueError(f"ì¡°íšŒëœ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. (ê¸°ê°„: {start_datetime} ~ {end_datetime})")
    
    # ë¶„ì„ ë° ì €ì¥
    report, result_text = analyze_and_save(db, news_articles, analysis_date)
    
    print(f"âœ… ë²¡í„° DB ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ: ë³´ê³ ì„œ ID={report.id}, ë‰´ìŠ¤ {len(news_articles)}ê°œ ë¶„ì„")
    
    return report, result_text
